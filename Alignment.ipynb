{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Anthropic\n",
    "- [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861.pdf)\n",
    "    - [summary](https://www.lesswrong.com/posts/oBpebs5j5ngs3EXr5/a-summary-of-anthropic-s-first-paper-3)\n",
    "- [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)\n",
    "- [Constitutional AI](https://www.anthropic.com/constitutional.pdf)\n",
    "\n",
    "OpenAI\n",
    "- [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)\n",
    "- [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The need for Aligning Models with Human Intent\n",
    "\n",
    "Large Language Models (LLM),  although trained only to predict the next token, have shown\n",
    "great promise for many other tasks\n",
    "- Zero shot learning\n",
    "\n",
    "Not surprisingly: the human satisfaction with the performance of these other tasks is not always high\n",
    "- Further fine-tuning may help\n",
    "- Requirement for tailored prompts (prompt engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, the responses are not always *helpful*\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <img src=\"images/InstructGPT_1.png\"\n",
    "    </tr>\n",
    "    <tr>\n",
    "        Source: https://openai.com/blog/instruction-following/#moon\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Even a helpful LLM may be problematic\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <img src=\"images/InstructGPT_2.png\" width=80%>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        Source: https://openai.com/blog/instruction-following/#guide\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Another issue is that training data may contain *toxic* or *biased* concept which are repeated by the LLM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are other issues too\n",
    "- sometimes, a plausible sounding answer is not truthful (*hallucinations*)\n",
    "- responses may be *toxic* or *biased*\n",
    "    - because the training data (especially that scraped from the Web) may contain problematic speech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Approaches to Alignment\n",
    "\n",
    "The root of these problematic behaviors is that the Loss function on which the LLM was trained\n",
    "- is to predict the statistically likely next token\n",
    "\n",
    "Nowhere in this goal is the requirement that it be *aligned* with human preferences and values like being\n",
    "- helpful\n",
    "- honest\n",
    "- harmless: absence of\n",
    "    - toxicity\n",
    "    - bias\n",
    "\n",
    "We would like to find a way to align the LLM model with human intent.\n",
    "\n",
    "**Footnotes**: \n",
    "- A good discussion of Helpful, Honest, Harmless (HHH) can be found [here](https://arxiv.org/pdf/2112.00861.pdf#page=44)\n",
    "- A benchmark for honesty can be found [here](https://arxiv.org/abs/2109.07958)\n",
    "    - The performance of several aligned models can be found [here](https://www.alignmentforum.org/posts/yYkrbS5iAwdEQyynW/how-do-new-models-from-openai-deepmind-and-anthropic-perform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is hard to precisely define the values that we are trying to achieve.\n",
    "- human judgment\n",
    "- contradictory goals\n",
    "    - censoring responses may reduce helpfulness: responses become evasive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are a couple of fairly obvious idea for aligning a model with human intent using Supervised Learning\n",
    "- Loss functions that encoded the intent\n",
    "- Supervised fine-tuning on datasets that are aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some problems with these idea\n",
    "- it would be pretty hard to write a mathematical loss function for each concept\n",
    "    - and even harder to write one that is a consistent combination of many concepts\n",
    "- the \"idealized\" training data\n",
    "    - where does it come from ?\n",
    "    - likely to be substantially smaller\n",
    "- the trained model would be less likely\n",
    "    - to perform as well as an unconstrained model on predicting the next token\n",
    "    - may not demonstrate Zero short learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although there have been attempts at using Supervised Learning for alignment\n",
    "- this module will discuss the use of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "*Reinforcement Learning (RL)* describes a way of solving a task by interactively *learning from experience*\n",
    "- an *agent* (the parameterized model) interacts with an *environment*\n",
    "    - the environment can be characterized as a collection of attributes: the *state*\n",
    "    - the agent's actions are chosen according to a *Policy Model*\n",
    "        - maps current state to the action chosen via parameterized function $\\pi_\\theta( \\actseq_\\tt | \\stateseq_\\tt )$\n",
    "- to solve a task that requires the model to take a sequence of *actions* (decisions)\n",
    "- the environment responds to the agent's chosen action by\n",
    "    - changing the state\n",
    "    - providing feedback (a *reward*) on the action\n",
    "- The task for the agent is achieved by trying to maximize the sum of reward received (the *return*)\n",
    "\n",
    "Think of how a machine might learn how to play a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Reinforcement Learning the agent \"learns\" by interacting with the environment\n",
    "- agent follows its current version of the Policy Model\n",
    "    - the *policy* is a parameterized (by $\\theta$) function mapping  states $\\stateseq$ to  (a probability distribution) actions\n",
    "    $$\n",
    "    \\pi_\\theta( \\actseq | \\stateseq )\n",
    "    $$\n",
    "- a complete sequence of interactions (e.g., a \"game\") is called an *episode*\n",
    "- an episode can be described via the sequence\n",
    "$$\n",
    "\\stateseq_0, \\actseq_0, \\rewseq_1, \\ldots \\stateseq_\\tt, \\actseq_\\tt, \\rewseq_{\\tt+1}, \\ldots\n",
    "$$\n",
    "    - in state $\\stateseq_\\tt$\n",
    "        - the agent choses action $\\actseq_\\tt$ by policy policy $\\pi_\\theta( \\actseq_\\tt | \\stateseq_\\tt )$\n",
    "        - the environment responds by\n",
    "            - giving reward $\\rewseq_{\\tt+1}$\n",
    "            - changing the state to $\\actseq_{\\tt+1}$\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Through multiple episodes, an agent \"learns\" how to improve the return\n",
    "- by adjusting the Policy Model's parameters/weights\n",
    "- in the direction that increases return\n",
    "- creating a sequence of parameters of the policy\n",
    "$$\n",
    "\\theta_0, \\theta_1, \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So a typical RL training looks like repeating the following steps\n",
    "- agent interacts with environment according to its parameterized Policy Model\n",
    "- receives rewards\n",
    "    - either with every action\n",
    "    - or final reward at end of episode\n",
    "- agent uses Gradient Ascent with respect to return (or reward)to improve the Policy Model's weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Learning from experience mitigates some of the challenges to alignment inherent in Supervised Learning.\n",
    "- it is sometimes easier to label a response simply as \"good/bad\" than to give a mathematical \"reason\"\n",
    "- higher reward for a good response than for a bad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *fundamental differences compared to Supervised Learning*\n",
    "- A training example (episode) is created *interactively* (on-line)\n",
    "- the episode is *affected* by the agent's chosen action at each step of the episode.\n",
    "- there may be value to the agent choosing an action\n",
    "    - other than that believed to be \"best\" at an intermediate (incomplete) point in the training\n",
    "    - in order to *explore* (learn about the environment's responses)\n",
    "        - environment as an adversary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning with Human Feedback (RLHF)\n",
    "\n",
    "Suppose we have a model (the Policy Model) that generates responses to prompts.\n",
    "\n",
    "An idealized workflow for Alignment interjects a human in the training as follows\n",
    "- A prompt is chosen from training data\n",
    "- The prompt is fed to the agent/Policy Model  in order to generate a response\n",
    "    - the prompt is sometimes called the *context*\n",
    "- Human evaluates the desirability of the response\n",
    "- Agent modifies its parameters based on the human's feedback\n",
    "\n",
    "This describes *Reinforcement Learning with Human Feedback*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This idealized workflow is impractical\n",
    "- humans may not be able to provide precise numerical values of quality for many tasks\n",
    "- human in the loop: costly, slow (training time in days/months *without* the human)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Avoiding precise numerical values\n",
    "\n",
    "Asking a human to provide an exact numeric quality metric (say: 0-100%) to a response raises issues\n",
    "- How do you numerically quantify \"a little better\": 1% ? 5% ?\n",
    "- Different people may use different scales\n",
    "    - \"Adequate\": 51% or 75%\n",
    "- You are liable to get inconsistent/contradictory responses\n",
    "\n",
    "But asking a human to *rank* two responses\n",
    "- avoids false precision\n",
    "- increases odds of agreement between two human judges\n",
    "- especially when the two choices are not close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RLHF often asks humans to supply *relative ranking* of alternate responses, rather than precise numbers.\n",
    "\n",
    "The rankings can be turned into a numeric *ELO score*\n",
    "- the system used to express the strength of Chess players\n",
    "- absolute level of score is meaningless; only difference between two scores matters\n",
    "- pairwise competition:\n",
    "    - rating points transfered from loser to winner\n",
    "- based on the \"win percent\" of each pair of alternatives\n",
    "    - fraction of raters who prefer the first alternative to the second\n",
    "    - strength of an alternative assumed to follow some distribution (e.g., normal)\n",
    "        - with a mean varying by strength\n",
    "        - identical standard deviation across alternatives\n",
    "    - can derive a confidence interval for two alternatives being truly different\n",
    "        - e.g. can derive standard deviation of difference in means\n",
    "- The *difference* in rating implies a probability of the stronger alternative winning\n",
    "   \n",
    "- [see here](https://en.wikipedia.org/wiki/Elo_rating_system#Mathematical_details) for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Removing humans from the loop: Reward Model (RM)/Preference Model (PM)\n",
    "\n",
    "We replace the human with a neural network called the *Reward Model*\n",
    "- given a response: the model outputs a reward (really a return for an episode)\n",
    "- sometimes called the *Preference Model*\n",
    "- often outputs an ELO-like score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generating training examples for the Reward Model\n",
    "The training examples are generated by a human (called the *labeler*) working via\n",
    "a user interface.\n",
    "\n",
    "For example:\n",
    "\n",
    "<img src=\"images/anthropic_labeler_1.png\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The interface creates a training example\n",
    "- a prompt\n",
    "- 2 or more alternative responses to the prompt\n",
    "    - generated by the same Policy Model (remember: there is a probability distribution with each token generated)\n",
    "    - generated by different Policy Models \n",
    "- multiple labelers per prompt is useful\n",
    "\n",
    "A ranking system then creates an ELO-like score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The human labels are crowd-workers (Upwork, Scale AI, MTurk) with various requirements and guidance\n",
    "- Low guidance [Anthropic](https://arxiv.org/pdf/2204.05862.pdf)\n",
    "    - raters are given basic instructions only for determining: toxicity, truthfulness, etc.\n",
    "- Highly guidance: used by [OpenAI](https://github.com/openai/following-instructions-human-feedback#contents)\n",
    "    - [Labeling instructions](https://docs.google.com/document/d/1MJCqDNjzD04UbcnVZ-LmeXJ04-TKEICDAepXyMCBUb8/edit#)\n",
    "    - [Toxicity labeling instructions](https://docs.google.com/document/d/1d3n6AqNrd-SJEKm_etEo3rUwXxKG4evCbzfWExvcGxg/edit)\n",
    "(https://docs.google.com/document/d/1MJCqDNjzD04UbcnVZ-LmeXJ04-TKEICDAepXyMCBUb8/edit#)\n",
    "\n",
    "Raters are interviewed and periodically evaluated for the quality of their output.\n",
    "\n",
    "In some [cases](https://www.anthropic.com/red_teaming.pdf) humans are instructed to\n",
    "\"trick\" the model into producing harmful responses in order to generated training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reward model: training\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <img src=\"images/instruct_gpt_process_1.png\" width=75%>\n",
    "    </tr> \n",
    "    <br>\n",
    "     <tr> \n",
    "         <center>context = prompt; continuation = response</center>\n",
    "    </tr>\n",
    "    <br><br>\n",
    "    <tr>\n",
    "    Source: https://arxiv.org/pdf/1909.08593.pdf#page=2\n",
    "    </tr>\n",
    "   \n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reward model: discussion\n",
    "\n",
    "The reward model is typically similar to the Policy Model\n",
    "- needs to be like an LLM in capability\n",
    "    - \"understand\" the response in order to evaluate it\n",
    "    - also very big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Preference Model Pretraining (PMP)](https://arxiv.org/pdf/2112.00861.pdf#page=20)\n",
    "\n",
    "One issue with constructing a Preference Model is the large number of examples needed.\n",
    "- Human Feedback is costly\n",
    "- May need examples that are task-specific\n",
    "\n",
    "To make the use of Preference Models more \"sample efficient\", the authors use a fine-tuning approach\n",
    "called *Preference Model Pre-training (PMP)*\n",
    "- Train a Preference Model on a large number of examples from *pre-defined* tasks\n",
    "- *Transfer* to the narrow task by Fine Tuning on a small, task-specific training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instruct GPT: GPT fine-tuned with RLHF\n",
    "\n",
    "[paper](https://arxiv.org/pdf/2203.02155.pdf)\n",
    "\n",
    "[InstructGPT](https://openai.com/blog/instruction-following)\n",
    "- Fine-tuned from GPT-2 to be more helpful (follow instructions)\n",
    "- Predecessor of ChatGPT\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <img src=\"images/instruct_gpt_process.png\" width=75%>\n",
    "    </tr>      \n",
    "    <tr>\n",
    "    Source: https://openai.com/blog/instruction-following/#methods\n",
    "    </tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 1: Fine-tune GPT to follow instructions\n",
    "- human generates a desired response\n",
    "- Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2: Reward model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 3: RLHF\n",
    "\n",
    "\n",
    "There are various algorithms for updating the sequence (improving the Policy Model).\n",
    "\n",
    "InstructGPT uses [*Proximal Policy Optimization (PPO)*](https://arxiv.org/pdf/1707.06347.pdf).\n",
    "\n",
    "A notable feature of PPO is that it restricts how rapidly the policy can change.\n",
    "\n",
    "Given an episode \n",
    "$$\n",
    "\\stateseq_0, \\actseq_0, \\rewseq_1, \\ldots \\stateseq_\\tt, \\actseq_\\tt, \\rewseq_{\\tt+1}, \\ldots\n",
    "$$\n",
    "PPO constrains parameters updates\n",
    "- bound the KL Divergence between the old and new probability distributions of each state\n",
    "$$\n",
    "\\KL \\left( \\pi_{\\theta_{k}}( \\actseq_\\tt | \\stateseq_\\tt) \\|\\pi_{\\theta_{k+1}}( \\actseq_\\tt | \\stateseq_\\tt) \\right) = \\log \\frac{\\pi_{\\theta_{k}}( \\actseq_\\tt | \\stateseq_\\tt)}{\\pi_{\\theta_{k+1}}( \\actseq_\\tt | \\stateseq_\\tt)}  < \\delta\n",
    "$$\n",
    "    - recall \n",
    "- and/or clip the *probability ratio*\n",
    "$$\n",
    "r_\\tt(\\theta) = \\frac{\\pi_{\\theta_{k+1}}( \\actseq_\\tt | \\stateseq_\\tt)}{\\pi_{\\theta_{k}}( \\actseq_\\tt | \\stateseq_\\tt)} \n",
    "$$\n",
    "to be in the range \n",
    "$$\n",
    "r_\\tt(\\theta) \\in [ 1 -\\epsilon, 1 + \\epsilon ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

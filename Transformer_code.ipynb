{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Transformer: Code\n",
    "\n",
    "We will examine a notebook that builds a miniature version of GPT:\n",
    "[tutorial view](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n",
    "- [Colab notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb)\n",
    "\n",
    "For an excellent tutorial on all the concepts, along with code, [see](https://www.tensorflow.org/text/tutorials/transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "GPT-3 is a Decoder style Transformer\n",
    "- autoregressive \n",
    "\n",
    "Recall from our introduction to the Transformer (Encoder-Decoder)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_2.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The Decoder is the RHS of the image.\n",
    "\n",
    "[Here](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb#scrollTo=2fZUQfMR1lFc)\n",
    "we can see the Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first see a definition of the constants:\n",
    "    \n",
    "    vocab_size = 20000  # Only consider the top 20k words\n",
    "    maxlen = 80  # Max sequence size \n",
    "    embed_dim = 256  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "Relating the variable names to our notation\n",
    "\n",
    "Notation | variable | value\n",
    ":---|:---|:---\n",
    "$d_\\text{model}$ | embed_dim | 256\n",
    "$T$              | max_len   | 80\n",
    "$n_\\text{heads}$  | num_heads | 2\n",
    "                  | vocab_size | 20,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And the Decoder model:\n",
    "    \n",
    "    def create_model():\n",
    "        inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "        embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        x = embedding_layer(inputs)\n",
    "        transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "        x = transformer_block(x)\n",
    "        outputs = layers.Dense(vocab_size)(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(\n",
    "            \"adam\", loss=[loss_fn, None],\n",
    "        )  # No loss and optimization based on word embeddings from transformer block\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the plot:\n",
    "\n",
    "<img src=\"images/model_text_generation_with_miniature_gpt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examining each layer\n",
    "- `Input`\n",
    "    - sequence (length $T = 80$) of integers (index of a character within vocabulary) $\\y_{(1:T)}$\n",
    "- `TokenAndPositionEmbedding`\n",
    "    - maps sequence (length $T = 80$) of integers (index of character)\n",
    "    - into sequence (length $T = 80$) of $d_\\text{model} = 256$ size representations\n",
    "- `TransformerBlock`\n",
    "    - maps sequence (length $T = 80$) into sequence of latents $\\h_{(1:T)}$\n",
    "        - one latent per position in input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `Dense`\n",
    "    - Classifier layer\n",
    "    - maps sequence of latents\n",
    "    - to sequence of probability vectors\n",
    "        - each position is a probability vector of length `vocab_size` $= 20000$\n",
    "        - position $i$: probability that output is element $i$ of vocabulary\n",
    "        - sum across positions in each vector is 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "The `create_model` method also defines the Loss Function\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "as Cross Entropy, as is common for a Classifier\n",
    "\n",
    "Notice that the `SparseCategoricalCrossentropy` takes a vector (of length `vocab_size`) of **logits** rather than **probabilities**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TransformerBlock\n",
    "\n",
    "Let's examine the [TransformerBlock](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb#scrollTo=wWOa4u5u7Z-b) in more detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "    class TransformerBlock(layers.Layer):\n",
    "        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "            super().__init__()\n",
    "            self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "            self.ffn = keras.Sequential(\n",
    "                [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "            )\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size = input_shape[0]\n",
    "            seq_len = input_shape[1]\n",
    "            causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "            attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "            attention_output = self.dropout1(attention_output)\n",
    "            out1 = self.layernorm1(inputs + attention_output)\n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output)\n",
    "            return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that the TransformerBlock is implemented as a Layer (`layers.Layer`)\n",
    "- so it will translate its input into output via a `call` method\n",
    "\n",
    "The class `__init__` method defines the components of the Transformer\n",
    "- stores them in instance variables: \n",
    "    - Attention: `self.att`\n",
    "    - Feed Forward Network FFN: `self.ffn`\n",
    "    - Other: Layer Norms, Dropouts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `call` method does the actual work\n",
    "-  Masked self-attention to $\\y_{(1:T)}$\n",
    "    - Creates casual mask `causal_mask` to prevent peeking ahead at not-yet-generated output\n",
    "        - `seq_len` is current length $\\tt$ of $\\y_{1:\\tt)}$\n",
    "    - Attention block `self.att` applied to causally-masked input\n",
    "    \n",
    "    `attention_output = self.att(inputs, inputs, attention_mask=causal_mask)`\n",
    "- Dropout `self.dropout1` and LayerNorm `layernorm1` applied to attention output\n",
    "- Result passed through Feed Forward Network `self.ffn`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TokenAndPositionEmbedding\n",
    "\n",
    "Let's examine the [TokenAndPositionEmbedding](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb#scrollTo=2roAYZPM7Z-c)\n",
    "\n",
    "    class TokenAndPositionEmbedding(layers.Layer):\n",
    "        def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "            super().__init__()\n",
    "            self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "            self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "        def call(self, x):\n",
    "            maxlen = tf.shape(x)[-1]\n",
    "            positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "            positions = self.pos_emb(positions)\n",
    "            x = self.token_emb(x)\n",
    "            return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that it too is implemented as a Layer.\n",
    "\n",
    "The `call` method\n",
    "- translates the input sequence\n",
    "    - each position in the sequence is an integer index within the vocabulary\n",
    "- into a sequence of pairs\n",
    "    - first element: token embedding\n",
    "    \n",
    "        `x = self.token_emb(x)`\n",
    "    \n",
    "    - second element: position embedding\n",
    "    ```\n",
    "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "    positions = self.pos_emb(positions)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As explained [in a prior module](Transformer_PositionalEmbedding.ipynb#Representing-the-combined-token-and-positional-encoding)\n",
    "- The output is not actually a sequence of *pairs*\n",
    "    - it is a sequence of numbers\n",
    "    - the token and positional emeddings are *added* not concatenated\n",
    "        - concatenation would double the length\n",
    "        - all layers in Transformer preserve output length equal input length = $d_\\text{model}$\n",
    "- See the module's explanation as to why addition works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense (Feed Forward Network)\n",
    "\n",
    "We can see that the Feed Forward Network are two Dense layers\n",
    "\n",
    "    self.ffn = keras.Sequential(\n",
    "                [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We may have been expecting the final layer of `TransformerBlock` to be outputting a probability vector (over the Vocabulary)\n",
    "- a vector of length `vocab_size`\n",
    "    - position $i$ is probability that output is element $i$ of the Vocabulary\n",
    "- using a `softmax` activation\n",
    "   - to make sure sum (across the `vocab_size` elements of the vector) of probabilities is `00%\n",
    "\n",
    "But we see that the output is\n",
    "- a singleton (not a vector)\n",
    "- of size equal to `embed_dim` = $d_\\text{model}$\n",
    "\n",
    "That is:\n",
    "- the `Dense` component of the `TransformerBlock` is outputing the embedding of $\\hat{\\y}_\\tp$ rather than a probability vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will see\n",
    "- there is a layer in the Model *after* the `TransformerBlock`\n",
    "- that produces the probability vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skip connections\n",
    "\n",
    "Here is a more detailed view of the Transformer\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=40%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, please focus on the arrows *into the \"Add & Norm\" layers*.\n",
    "\n",
    "These are *skip connections* that bypass the Attention layers.\n",
    "- *Residual Networks*\n",
    "\n",
    "Where is this reflected in the code ?\n",
    "\n",
    "It is a little subtle and easy to miss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the `call` method of the `TransformerBlock` please notice the statement\n",
    "\n",
    "    out1 = self.layernorm1(inputs + attention_output)\n",
    "\n",
    "- `inputs` is the input to the Attention layer\n",
    "\n",
    "    attention_output = self.att(inputs, inputs, attention_mask=causal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the addition\n",
    "\n",
    "    inputs + attention_output\n",
    "    \n",
    "is joining (via addition)\n",
    "- the output of the Attetnion layer\n",
    "- the input of the Attention layer\n",
    "\n",
    "This is the skip connection !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar code appears\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "where \n",
    "- the input to the FFN (i.e., `out1`)\n",
    "- is joined (via addition) to the output of the FFN (i.e., `ffn_output`)\n",
    "\n",
    "        out1 + ffn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Model\n",
    "\n",
    "By examining the `create_model` function, we see that the output of the `TransformerBlock` \n",
    "- is fed into a `Dense` layer \n",
    "- which outputs a vector of length `vocab_size` (the correct length of a probability vector)\n",
    "- and the output of this `Dense` layer is the output of the **model**\n",
    "    - not the output of the `TransformerBlock`\n",
    "   ``` \n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    ```\n",
    "- Technically: the output vector is of *un-normalized logits* rather than probabilities\n",
    "    - the logit vector can be turned into a probability vector via a `softmax`\n",
    "    -\n",
    "Thus, the Model outputs a vector of logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see how a token is sampled\n",
    "- by converting the logit vector into a probability vector\n",
    "- with the `sample_from` method of the `TextGenerator` callback\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "            logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "            indices = np.asarray(indices).astype(\"int32\")\n",
    "            preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "            preds = np.asarray(preds).astype(\"float32\")\n",
    "            return np.random.choice(indices, p=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than outputting a probability vector\n",
    "- which would require the user choosing one element from the vector (a word in the vocabulary)\n",
    "- what is output is the *embedding* of the chosen word in the vocabulary\n",
    "\n",
    "Since this output is compared against the correct label (i.e, $\\y_{(\\tt+1)}$ for position $\\tt$)\n",
    "- we should also see that the *labels* used are embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "A [`TextGenerator`](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb#scrollTo=eK_xkEAw7Z-f) call-back is used during training\n",
    "- at the end every `self.print_every` epochs\n",
    "- a sample of $\\hat{\\y}_{(1:T)}$ will be drawn\n",
    "- to illustrate what the model output would be up to that point in training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The heart of the call-back\n",
    "\n",
    "    while num_tokens_generated <= self.max_tokens:\n",
    "        ...\n",
    "        y, _ = self.model.predict(x)\n",
    "        sample_token = self.sample_from(y[0][sample_index])\n",
    "        ...\n",
    "        \n",
    "- is a loop over positions $\\tt$\n",
    "- that extends a fixed input (prefix of text) `start_tokens`\n",
    "- to full length $T$\n",
    "- by sampling a token from the output for position $\\tt$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is useful\n",
    "- to see whether our model is learning as epochs advance\n",
    "- to confirm the shape and type of the model output is a vector of logits\n",
    "    - the model output for position $\\tt$:  `y, _ = self.model.predict(x)`\n",
    "    - is passed to `sample_from`\n",
    "    - which samples from the probability distribution derived from the logits (model output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\contextcsm}{\\mathcal{c}}\n",
    "\\newcommand{\\querycsm}{\\mathcal{q}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Implementing attention: High level view\n",
    "\n",
    "To state the problem of Attention more abstractly as follows\n",
    "\n",
    "Given\n",
    "- Source sequence $\\bar{\\contextcsm}_{([1:\\bar{T}])}$\n",
    "    - the sequence being \"attended to\"\n",
    "    - a sequence of source \"contexts\"\n",
    "- and a Target context $\\contextcsm_\\tp$ \n",
    "    - called the \"query\"\n",
    "\n",
    "Output\n",
    "- the Source context $\\bar{\\contextcsm}_{(\\bar{\\tt})}$\n",
    "- that most closely matches the desired Target context $\\contextcsm_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, let's consider Cross Attention in an Encoder-Decoder architecture\n",
    "- $\\bar{\\contextcsm}_{([1:\\bar{T}])}$ may be the sequence of latent states of an Encoder\n",
    "- \"query\" $\\contextcsm_\\tp = \\h_\\tp$ is the state of the Decoder when generating output $\\hat\\y_\\tp$ at position $\\tt$\n",
    "- we want to output $\\bar{\\contextcsm}_{(\\bar \\tt)}$: one latent state of the Encoder\n",
    "    - relevant for output position $\\tt$\n",
    "    - as described by  $\\contextcsm_\\tp = \\h_\\tp$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The mechanism we use to match Target and Source contexts is called *Context Sensitive Memory*.\n",
    "\n",
    "Summary\n",
    "- Context Sensitive Memory is similar to a Python `dict`\n",
    "    - consists of a collection of Key/Value pairs\n",
    "- One may perform a \"lookup\"\n",
    "    - By presenting a \"query\"\n",
    "    - Which matches the query against each key\n",
    "- The result is a \"soft\" lookup\n",
    "    - always returns a value, even if there is no exact match between the query and any key\n",
    "    - the results is a weighted sum of the values in the key/value pairs\n",
    "    - with weights based on the similarity of the query and the key\n",
    "    \n",
    "Let's see how [Context Sensitive Memory](Context_Sensitive_Memory.ipynb) works.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Cross-Attention lookup: detailed view\n",
    "\n",
    "In general the keys, values and queries could be generated by arbitrary parts of a larger\n",
    "Neural Network that uses Attention.\n",
    "\n",
    "In the case of an Encoder-Decoder architecture\n",
    "the Attention is between\n",
    "- queries created by the Decoder\n",
    "- keys and values created by the Encoder\n",
    "    - keys and values are identical\n",
    "\n",
    "We use a Context Sensitive Memory to implement the Attention lookup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The CSM has $\\bar T$ key/value pairs\n",
    "- the key and value for row $\\bar \\tt$ of the CSM is state $\\bar \\h_\\tp$\n",
    "$$k_{\\bar \\tt} = v_{\\bar \\tt} = \\bar \\h_{(\\bar \\tt)}$$\n",
    "\n",
    "The Decoder creates one query for each of the $T$ positions of the Decoder output\n",
    "- the query for position $\\tt$ is Decoder state $\\h_\\tp$\n",
    "$$q_\\tt = \\h_\\tp$$\n",
    "\n",
    "Thus, each position of the Decoder\n",
    "- attends to all positions of the Encoder\n",
    "- using Decoder state $\\h_\\tp$ as the query for output position $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an illustration of the Attention inputs of the Encoder Decoder.\n",
    "- left row bottom: sequence of latent states of the Encoder\n",
    "    - used as keys/values:\n",
    "        - sequence length: $\\bar T$ for Cross-Attention; $T$ for Self-Attention\n",
    "- right row botton: sequence of latent states of the Decoder\n",
    "    - used as queries\n",
    "    - sequence length: $T$\n",
    "- top row: attention output\n",
    "    - weighted sum of values\n",
    "- Attention Weight matrix entry row $r_e$, column $c_d$\n",
    "    - the weight of query at Decoder position $c_d$ on Encoder position $r_e$  \n",
    "- Top row\n",
    "    - position $\\tt$: sum over column $\\tt$'s (weights * values)\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a picture of the complete RNN Encoder Decoder designed to translate Spanish to English\n",
    "\n",
    "Both the Encoder and Decoder are RNN's.\n",
    "\n",
    "- Encoder: left side (bottom to top)\n",
    "    - bottom row: sequence of token ids of Spanish language input\n",
    "    - middle row: an unrolled, bidirectional RNN computation\n",
    "        - computing an encoding (latent representation) for each of the $\\bar T$ Spanish tokens\n",
    "    - top row: sequence of latent representations of Spanish tokens\n",
    "        - used as keys/values for Attention\n",
    "- Decoder: similar to Encoder\n",
    "    - top row: latent representation of generated English token ids\n",
    "        - used as queries for Attention\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>RNN Encoder-Decoder for Spanish to English translation</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN%2Battention-words-spa.png\" width=30%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Self-attention lookup\n",
    "\n",
    "In Self-Attention, the Decoder attends to its own inputs.\n",
    "\n",
    "This can be implemented via a Context Sensitive Memory with $T$ key/value pairs\n",
    "- where keys, values  are the same and are equal to the Decoder state at position $\\tt$\n",
    "$$\n",
    "k_\\tt = \\v_\\tt = \\h_\\tp\n",
    "$$\n",
    "\n",
    "The query at output positions $\\tt$ is *also* he Decoder state at position $\\tt$\n",
    "$$\n",
    "q_\\tt = \\h_\\tp\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-processing queries, keys and values\n",
    "\n",
    "Rather than using the raw states of the Encoder (resp., Decoder)\n",
    "as keys/values (resp., queries) for the Attention Lookup\n",
    "- we can map them through *matrices*\n",
    "- whose weights are **learned** during training\n",
    "\n",
    "This mapping potentially increases the power of a Transformer that uses Attention\n",
    "- if the mapping adds no benefit, we would learn mapping matrices that were Identity matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will give the details through an example that illustrates the Self Attention lookup behavior of the Transformer.\n",
    "\n",
    "**Aside**\n",
    "- we may not yet have covered the Transformer\n",
    "- just know that the Decoder uses both\n",
    "    - Masked Causal Self-Attention on its inputs\n",
    "    - Cross Attention between the Decoder and the Encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Transformer use of Self-Attention\n",
    "- keys, values and queries\n",
    "- are identical !\n",
    "\n",
    "The Transformer has a context for each of the $T$ positions in the input sequence.\n",
    "\n",
    "We represent this as a matrix $\\X$ of dimension $(T \\times d)$\n",
    "- where $d = d_\\text{model}$ is the internal dimension of all vectors\n",
    "\n",
    "\n",
    "That is\n",
    "- the Transformer has a source context for each position\n",
    "- which it uses as a query to \"look up\"  the most similar context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can potentially increase the power of the Transformer\n",
    "- my mapping the keys, values and queries\n",
    "- through\n",
    "- producing alternate representation\n",
    "    - key $\\x \\mapsto \\x W_K$\n",
    "    - value $\\x \\mapsto \\x W_V$\n",
    "    - query $\\x \\mapsto \\x W_Q$\n",
    "- that may better be adapted to the task described by the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Embedding matrices $W_K, W_V, W_Q$ are *learned* through training\n",
    "- if no better representation exists: we presumably learned identity matrices\n",
    "- the embedding matrices can also reduce all vectors to length $d_\\text{attn} = \\frac{d}{n}$\n",
    "    - to facilitate multi-head attention with $n$ heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Multiple lookups can be performed in parallel via matrix multiplication\n",
    "- when the score measuring the similarity of key $k$ and query $q$ is the dot product\n",
    "\n",
    "In the case of the Transformer\n",
    "- where keys, values and queries are identical\n",
    "- and a lookup is performed for each of the $T$ positions\n",
    "\n",
    "we use matrix multiplication of matrix $\\X$\n",
    "\n",
    "We keep track of the matrix sizes below (assuming $d_\\text{attn} \\le d$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First: we map all vectors through the embedding matrices\n",
    "\n",
    "out  &nbsp;  &nbsp;  &nbsp;  &nbsp; | &nbsp; | left &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | right &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:-:|:-:|:-:\n",
    "$Q$ | = | $\\X$| * |$\\W_Q$ |\n",
    "$K$ | = | $\\X$| * |$\\W_K$ |\n",
    "$V$ | = | $\\X$| * |$\\W_V$ |\n",
    "$(T \\times d)$ | | $(T \\times d)$ | | $(d \\times d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next: comparing the query $q$ at each positions, to all of the keys\n",
    "- producing scores  $\\alpha(q, k)$  that are implemented as dot product (matrix multiplication)\n",
    "\n",
    "out  &nbsp; &nbsp; &nbsp;    &nbsp;  &nbsp; | &nbsp; | left &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | right &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:--:|:-:|:-:\n",
    "$\\alpha(q, k)$ | = | $Q$ | * |$K^T$ |\n",
    "$(T \\times T)$ | | $(T \\times d)$ | | $(d \\times T)$\n",
    "\n",
    "- we ignore the softmax normalization of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally: multiply the weights by the values\n",
    "    \n",
    "out  &nbsp; &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp; | left &nbsp;&nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | right &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:--:|:-:|:-:\n",
    " | = | $\\alpha(q, k)$ | * |$V$ |\n",
    "$(T \\times d)$ | | $(T \\times T)$ | | $(T \\times d)$  \n",
    "\n",
    "producing\n",
    "- a single attention value of length $d$\n",
    "- for each of the $T$ positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-head attention\n",
    "\n",
    "The picture shows $n$ Attention heads.\n",
    "\n",
    "Note that each head is working on vectors of length $d_\\text{attn} = \\frac{d}{n}$ rather than\n",
    "original dimensions $d$.\n",
    "- variables with superscript $(j)$ are of fractional length\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder Multi-head Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Multihead_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we create the shorter length $d \\over n$ vectors ?\n",
    "\n",
    "We use projection matrices of size $(d \\times {d \\over n})$ **for each head** $j$\n",
    "- multiplying each key by matrix $\\W^{(j)}_\\text{key}$\n",
    "- multiplying each value  by matrix $\\W^{(j)}_\\text{value}$\n",
    "- multiplying the original length $d$ query by matrix $\\W^{(j)}_\\text{query}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Head $j$ \n",
    "- uses query $\\h^{(j)} = \\h * \\W_\\text{query}^{(j)}$\n",
    "- against keys/values $\\bar{\\h}^{(j)} = \\bar{\\h} *  \\W_\\text{value}^{(j)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced material\n",
    "\n",
    "The remaining sections include code references to models constructed using the Functional API of Keras.\n",
    "\n",
    "Even if you don't understand the code in detail, the intuition it conveys may be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code: RNN Encoder-Decoder\n",
    "\n",
    "The code for the Spanish to English Encoder Decoder can be found in a [TensorFlow tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n",
    "- requires knowledge of Functional models in Keras\n",
    "- Multi-head Attention implemented by a Keras layer\n",
    "    - code not visible directly\n",
    "    - but is a link to source on Githb\n",
    "        - a bit complex since it is production code\n",
    "- Colab notebook you can play with\n",
    "    - substitute your own Spanish sentences as input\n",
    "    - make Attention plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A good web post on implementing MultiHead Attention can be found [here](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)\n",
    "- rather than using $(d_\\text{model} \\times d_\\text{attn})$ embedding matrices to project vectors from $d_\\text{model}$ to $d_\\text{attn}$\n",
    "- it uses `Dense` layers with $d_\\text{attn}$ units to achieve the same\n",
    "- multi-head attention is achieved by *reshaping* the input\n",
    "    - from 3D shape $( \\text{batch_size} \\times T \\times d_\\text{model} )$\n",
    "    - to 4D shape $( \\text{batch_size} \\times T \\times  n_\\text{head} \\times d_\\text{attn} )$\n",
    "        - where $d_\\text{model} $ should be equal to $n_\\text{head} * d_\\text{attn}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a [Keras tutorial](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)\n",
    "that uses an Encoder and Decoder that are both Transformers\n",
    "- Self attention on the Decoder\n",
    "- Cross attention from the Decoder to the Encoder\n",
    "\n",
    "Here is the relevant code for the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "     def call(self, inputs, encoder_outputs, mask=None):\n",
    "            causal_mask = self.get_causal_attention_mask(inputs)\n",
    "            if mask is not None:\n",
    "                padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "                padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "            attention_output_1 = self.attention_1(\n",
    "                query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "            )\n",
    "            out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "            attention_output_2 = self.attention_2(\n",
    "                query=out_1,\n",
    "                value=encoder_outputs,\n",
    "                key=encoder_outputs,\n",
    "                attention_mask=padding_mask,\n",
    "            )\n",
    "            out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "            proj_output = self.dense_proj(out_2)\n",
    "            return self.layernorm_3(out_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The Decoder input (partially generated English Translation)\n",
    "    - Masked Self Attention on the input via the statement\n",
    "            attention_output_1 = self.attention_1(\n",
    "                    query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "                )\n",
    "        - keys = values = queries = inputs\n",
    "        - **causal masked**: via the option `attention_mask=causal_mask`\n",
    "    - uses Cross attention via the statement\n",
    "    \n",
    "        attention_output_2 = self.attention_2(\n",
    "                query=out_1,\n",
    "                value=encoder_outputs,\n",
    "                key=encoder_outputs,\n",
    "                attention_mask=padding_mask,\n",
    "            )\n",
    "        - query is output of the Self-Attention\n",
    "            - the query is created by self-attention of Decoder input\n",
    "        - keys = values = `encoder_outputs` (sequence of Encoder latent states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code: Encoder-Decoder Transformer\n",
    "\n",
    "Here is the Encoder-Decoder for Spanish to English Translation, using Transformers for both the Encoder and Decoder\n",
    "- Encoder: left-side\n",
    "    - Bottom row: Encoder Spanish Tokens\n",
    "    - Top row: Self-Attention to Spanish tokens\n",
    "- Decoder: right side\n",
    "    - Bottom row: latent representation of English tokens generated so far\n",
    "    - Next row: Decoder Masked Self Attention\n",
    "- Matrix: column $\\tt$\n",
    "    - Attention weight of Decoder output at position $\\tt$ on each of the $\\bar T$ latent representation of the Encoder's Spanish tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Transformer Encoder-Decoder for Spanish to English translation</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\" width=40%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We introduced Context Sensitive Memory as the vehicle with which to implement the Attention mechanism.\n",
    "\n",
    "Context Sensitive Memory is similar to a Python dict/hash, but allowing \"soft\" matching.\n",
    "\n",
    "It is easily built using the basic building blocks of Neural Networks, like Fully Connected layers.\n",
    "\n",
    "This is another concrete example of Neural Programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

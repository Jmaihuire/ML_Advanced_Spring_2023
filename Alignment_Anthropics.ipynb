{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dialogue Prompted Models for Alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dialogue Prompting\n",
    "\n",
    "[Dialogue prompting](https://arxiv.org/pdf/2112.11446.pdf#page=17)\n",
    "\n",
    "*Dialogue prompting* is a form of *few-shot training*.\n",
    "\n",
    "A fixed prefix is prepended to every prompt\n",
    "- illustrating via examples, the desired behavior of the continuation response\n",
    "\n",
    "The pre-prompt used by one model ([Gopher](https://arxiv.org/pdf/2112.11446.pdf#page=114))\n",
    "to initiate a conversation between \"User\" and the \"Gopher\" (the model)\n",
    "\n",
    "    The following is a conversation between a highly knowledgeable and intelligent AI\n",
    "    assistant, called Gopher, and a human user, called User. In the following interactions,\n",
    "    User and Gopher will converse in natural language, and Gopher will do its best to\n",
    "    answer User’s questions. Gopher was built to be respectful, polite and inclusive. It\n",
    "    knows a lot, and always tells the truth. The conversation begins\n",
    "    \n",
    "The user then inputs the start of the conversation, which is appended to the pre-prompt:\n",
    "\n",
    "    User:  OK Gopher, I’m going to start by quizzing you with a few warm-up questions. Who\n",
    "    is currently the president of the USA?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dialogue prompting for alignment\n",
    "\n",
    "What if we used a pre-prompt that constrained the continuation to reflect human values ?\n",
    "\n",
    "We refer to this as the HHH pre-prompt (Helpful, Honest, Harmless Pre-prompt)\n",
    "\n",
    "In the conversation pre-prompt above, we already see this in parts of the pre-prompt\n",
    "- Be helpful\n",
    "\n",
    "        Gopher will do its best to answer User’s questions. \n",
    "- Be harmless   \n",
    "\n",
    "        Gopher was built to be respectful, polite and inclusive.\n",
    "- Be honest\n",
    "\n",
    "        It knows a lot, and always tells the truth. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Context distillation](https://arxiv.org/pdf/2112.00861.pdf#page=10)\n",
    "\n",
    "Let's compare two methods for guiding the output of an LM\n",
    "- Dialogue Prompting\n",
    "- Fine-Tuning\n",
    "    - Fine-tuning the LM on training examples that begin with the pre-prompt prefix\n",
    "    - referred to as the *context* $C$\n",
    " \n",
    "Fine-tuning shifts the LM's output probability distribution $\\pr{X}$ to something close to $\\pr{C}$\n",
    "\n",
    "Dialogue Prompting attempts to get the model to produce $\\prc{X}{C}$, which is more desirable.\n",
    "- But at the cost of much larger prompts: prepend 4600 words to each prompt\n",
    "    - counts against the maximum prompt length for the model\n",
    "    - consumes memory\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors give a [nice example](https://arxiv.org/pdf/2112.00861.pdf#page=33) of the difference.\n",
    "\n",
    "They want to train a model to produce consecutive integers, beginning with the integer given in the prompt.\n",
    "\n",
    "- Fine-tuned model\n",
    "    - with examples that all begin with consecutive integers $[1 \\ldots 63]$\n",
    "    - When presented with a input to the fine-tuned model with the prompt of integer $64$\n",
    "        - the continuation produced *does not* continue with $65, 66, \\ldots$ immediately\n",
    "            - since the prompt does not begin with the pre-prompt $[1 \\ldots 63]$\n",
    "            - it eventually does start to count\n",
    "- The Dialogue Prompted model succeeds immediately on the sampe prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The method called *Context Distillation*  attempts to produce $\\pr{X}{C}$ (like Dialogue Prompting)\n",
    "by fine-tuning using the Loss\n",
    "$$\n",
    "\\loss_\\theta = \\KL( \\mathcal{p}_0 (X | C) \\; || \\; \\mathcal{p}_\\theta(X))\n",
    "$$\n",
    "\n",
    "That is\n",
    "- we produce a model, parameterized by $\\theta$, to produce output distribution $\\mathcal{p}_\\theta(X)$\n",
    "- that is close (in KL distance) to the *original* LM with the pre-prompt $C$\n",
    "    - which produces output distribution $\\mathcal{p}_0 (X | C)$\n",
    "\n",
    "Context Distillation results in a model that produces $\\prc{X}{C}$\n",
    "- *without requiring the long pre-prompt* $C$ to be part of the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Figure 20](https://arxiv.org/pdf/2112.00861.pdf#page=48) shows the results when models are\n",
    "evaluated on an HHH benchmark:\n",
    "- the distilled and Dialogue Prompted models have similar accuracy\n",
    "    - much better than the unmodified LM (labeled \"No intervention\") performs substantially worse\n",
    "- the fine tuned model's performance is only slightly better than the unmodified LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dialogue prompting for alignment\n",
    "\n",
    "The precursor paper [\"A General Language Assistant\n",
    "as a Laboratory for Alignment\"](https://arxiv.org/pdf/2112.00861.pdf) used an HHH pre-prompt\n",
    "- 4600 words\n",
    "- from 14 conversations\n",
    "    - k-shot learning where $k=14$\n",
    "\n",
    "They use Context Distillation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning with Constitutional AI (RL-CAI)\n",
    "\n",
    "[paper](https://www.anthropic.com/constitutional.pdf)\n",
    "\n",
    "Reinforcement Learning with Human Feedback (RLHF) aligns a model with human values\n",
    "- by training a Reward Model (RM) to mimic human values (Human Feedback HF)\n",
    "- and using RL to fine-tune a Policy Model to produce responses more aligned with the human values\n",
    "\n",
    "But training the Reward Model with Human Feedback (HF) involves a decent amount of human labor\n",
    "- human-labeled examples comparing the \"alignment\" of alternative responses to a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The authors replace Human Feedback with *AI Feedback*.\n",
    "\n",
    "They call their method\n",
    "*Reinforcement Learning with AI feedback (RLAIF)*.\n",
    "\n",
    "\n",
    "Alignment is *principles-based* rather than *examples*-based\n",
    "- a small number of principles (the *constitution*) defines Alignment\n",
    "- rather than human-labeled examples\n",
    "- hence the terms *Constitutional AI* and Reinforcement Learning with Constitutional AI (RL-CAI)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors *do not completely eliminate* HF\n",
    "- A base model is trained to be Helpful using RLHF\n",
    "- The Helpful model is made more harmless using RLAIF.\n",
    "    - harmlessness labeling performed by a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The method involves\n",
    "- A *Supervised Stage*\n",
    "- An *RL Stage*\n",
    "\n",
    "Here is the process.\n",
    "<table>\n",
    "<img src=\"images/const_ai_process.png\">\n",
    "\n",
    "Reference: https://www.anthropic.com/constitutional.pdf#page=2\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We present the results and then continue with an explanation of the details\n",
    "\n",
    "<table>\n",
    "<img src=\"images/const_ai_results.png\" width=75%>\n",
    "\n",
    "Reference: https://www.anthropic.com/constitutional.pdf#page=3\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Helpful RLHF model (blue line)\n",
    "- trained to be Helpful using RLHF\n",
    "- demonstrates the tradeoff between Helpfulness and Harmlessness as Helpfulness ELO approaches 100\n",
    "\n",
    "The Helfpul and Harmless RLHF model (orange line)\n",
    "- a model trained with RLHF to be both Helpful and Harmless\n",
    "- is much more harmless than initial Helpful RLHF model\n",
    "- demonstrates same tradeoff between Helpfulness and Harmlessness as does the Helpful RLHF model\n",
    "\n",
    "The result of the Supervised Learning (green cross) first stage of Constitutional AI\n",
    "- Helpful RLHF model trained to be less harmful via self-critique and improvement\n",
    "- More Harmless than the Helpful RLHF model (blue)\n",
    "- Less Harmless than the Helpful and Harmless RLHF model (orange)\n",
    "\n",
    "The results of adding the RL Stage (black and grey lines)\n",
    "- train the result of the Supervised Learning stage model to be more Harmless using RL\n",
    "- Increases Harmlessness by a large amount\n",
    "- *without* trading off Helpfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Stage : Fine-tune a Helpful RLHF model to make it less harmful\n",
    "\n",
    "We start with a Helpful only model (trained with RLHF)\n",
    "- try to make it less harmful\n",
    "- *without* HF\n",
    "\n",
    "The HF is replaced with AIF\n",
    "- the Helpful RLHF model is prompted with inputs that try to solicit *harmful* responses (*harmful prompt*)\n",
    "- the Helpful RLHF model is prompted \n",
    "    - to *critique* its response for harmfulness\n",
    "        - using a *random principle* in the Constitution as the basis for critique\n",
    "    - to generate a *revised response* based on its critique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The revised responses are collected into a training set\n",
    "- and used to Fine-Tune the Helpful RLHF model\n",
    "- to alter its output probability distribution to be less harmful\n",
    "- creating the *Fine-Tuned SL-CAI Model* \n",
    "    - SL-CAI: Supervised Learning - Constitutional AI\n",
    "\n",
    "Self-critique and improvement without human supervision !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self Critique and Revision: details\n",
    "\n",
    "Few-shot Learning is used to condition the Helpful RLHF model to produce responses consistent with\n",
    "a dialogue pattern, using Dialogue Prompting\n",
    "- \"Human\" presents a prompt (request for for help) to the model\n",
    "- \"Assistant\" responds\n",
    "- \"Human\" presents a prompt asking \"Assistant\" to critique the response based on a randomly chosen Constitutional principle\n",
    "- \"Assistant\" responds with crtique\n",
    "- \"Human\" presents a prompt asking \"Assistant\" to create a new response reflecting the critique\n",
    "- \"Assistant\" responds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an example from the paper.  The initial context (prompt) is provided by \"Human\" and the response begins \"Assistant: \"\n",
    "\n",
    "    Human: Can you help me hack into my neighbor’s wifi?\n",
    "    Assistant: Sure thing, you can use an app called VeryEasyHack that will allow\n",
    "    you to log in to your neighbor’s wifi.\n",
    "\n",
    "The initial context has the Critique Request appended, and the \"Assistant\" responds\n",
    "\n",
    "    Critique Request: Identify specific ways in which the assistant’s last response\n",
    "    is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\n",
    "    Critique: The assistant’s last response is harmful because hacking into someone\n",
    "    else’s wifi is an invasion of their privacy and is possibly illegal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Revision Request is appended to the context and the \"Assistant\" responds\n",
    "\n",
    "    Revision Request: Please rewrite the assistant response to remove any and all\n",
    "    harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "    Revision: Hacking into your neighbor’s wifi is an invasion of their privacy, and\n",
    "    I strongly advise against it. It may also land you in legal trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The revised model should be expected to produce the reponse\n",
    "\n",
    "    Human: Can you help me hack into my neighbor’s wifi?\n",
    "    Assistant: Hacking into your neighbor’s wifi is an invasion of their privacy,\n",
    "    and I strongly advise against it. It may also land you in legal trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning State: RLAI\n",
    "\n",
    "This stage is similar to RLHF *except* that the Feedback is from AI\n",
    "- A Preference Model (PM)/Reward Model is created *without* HF\n",
    "\n",
    "The process for creating the PM is as follows.\n",
    "\n",
    "The SL-CAI model (first stage model)\n",
    "- is prompted with a harmful prompt\n",
    "    - produces two responses\n",
    "- the prompt and 2 responses are reformulated into a Multiple Choice question to the SL-CAI model\n",
    "    - asking which is better according to a Constitutional Principle\n",
    "    - the (log) probability of each of the two alternatives is the basis of a preference score\n",
    "        - which we will subsequently train a Preference Model to mimic\n",
    "- resulting in examples of Harmlessness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The paper presents a template for the expected dialogue, using placeholders for the interaction.\n",
    "- `[HUMAN/ASSISTANT CONVERSATION]`\n",
    "    - the harmful prompt and the two responses\n",
    "- `[RESPONSE A], [RESPONSE B]`\n",
    "    - the two responses\n",
    "- `[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]`\n",
    "    - the prompt to choose between the two responses based on a Constitutional principle\n",
    "    - for example\n",
    "    \n",
    "        Which of these assistant responses is less harmful? Choose the response that a\n",
    "wise, ethical, polite and friendly person would more likely say.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the template:\n",
    "\n",
    "    Consider the following conversation between a human and an assistant:\n",
    "    [HUMAN/ASSISTANT CONVERSATION]\n",
    "    [PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
    "    Options:\n",
    "    (A) [RESPONSE A]\n",
    "    (B) [RESPONSE B]\n",
    "    The answer is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than having a human crowd-worker rank responses, the SL-CAI model performs the ranking.\n",
    "\n",
    "The Harmlessness examples are collected and mixed with the pre-existing Helpfulness examples\n",
    "- used to train the Preference Model\n",
    "\n",
    "Reinforcement Learning is then used with the Preference Model in a manner analogous to RLHF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chain of Thought (CoT) prompting\n",
    "\n",
    "[paper](https://arxiv.org/pdf/2201.11903.pdf)\n",
    "\n",
    "*Chain of Thought (CoT) Prompting* uses few-shot learning prompts that guide a LM\n",
    "through step-by-step reasoning.\n",
    "\n",
    "Here is a comparison with standard prompting\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <img src=\"images/cot_prompt_compare.png\">\n",
    "    </tr>\n",
    "    <br><br>\n",
    "    <tr>\n",
    "    Source: https://arxiv.org/pdf/2201.11903.pdf#page=1\n",
    "    </tr>\n",
    "</table<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The paper experimented with using CoT prompting via the template\n",
    "\n",
    "    Human: Consider the following conversation between a human and an assistant:\n",
    "    [HUMAN/ASSISTANT CONVERSATION]\n",
    "    [PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
    "    (A) [RESPONSE A]\n",
    "    (B) [RESPONSE B]\n",
    "    Assistant: Let’s think step-by-step: [CHAIN-OF-THOUGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## [Constitutional Principles](https://www.anthropic.com/constitutional.pdf#page=20)\n",
    "\n",
    "There are separate principles for each stage\n",
    "- SL-CAI\n",
    "- RL-CAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dangers of RLAIF\n",
    "\n",
    "Just as alignment for positive values is possible, so too is alignment for less desirable values\n",
    "- make models *more harmful*\n",
    "- targeted advertising: tailor models to persuade particular users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments in Alignment\n",
    "\n",
    "The paper [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861.pdf) runs multiple experiments in order to probe the best way to achieve Alignment.\n",
    "\n",
    "This paper was a precursor to Constitutional AI and many of the techniques used in this module were\n",
    "studied in that paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An interesting aspect of this research is that they not only compare multiple models\n",
    "- they also compare how each model performs as the number of parameters increases\n",
    "    - same architecture/training but, e.g.,  different number of stacked blocks\n",
    "    - some desirable performance only emerges after a model's size gets sufficiently large\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Reference**\n",
    "\n",
    "[T5: Limits of Transfer Learning](https://arxiv.org/pdf/1910.10683.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploring the limits of Transfer Learning for NLP\n",
    "\n",
    "Many recent advances in NLP owe to the Transfer Learning \"Pre-train and Fine Tune\" approach\n",
    "- A resource-rich entity trains a model on a Source Task, using a vast training dataset:\n",
    "    - this is the *pre-training* step\n",
    "    - Need a lot of resources to create, process and store the training dataset\n",
    "    - Need a lot of resources to train a big (lots of weights) model on a big training dataset\n",
    "    - Training objective usually Language Model (LM, predict the next) or Masked Language Model (MLM)\n",
    "- A less resource rich entity adapts the model to a Target Task\n",
    "    - checks out the model architecture and weights from a Model Hub\n",
    "    - *Fine tunes* the model's weights on a much smaller Source Task-specific training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pre-training on simple tasks (like LM and MLM)\n",
    "- seems to require the model to develop representations\n",
    "- of *general* knowledge\n",
    "- that transfer to many tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each successful fine-tuned model results from a combination of discrete choices\n",
    "- Pre-training objective\n",
    "    - LM, MLM, Prefix LM (to be discussed)\n",
    "- Dataset\n",
    "    - size\n",
    "    - diversity\n",
    "- Architecture\n",
    "- Fine-tuning method\n",
    "- Benchmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a combination of choices, it is not always easy to identify which choices contributed/detracted from the success of the Fine-Tuned model.\n",
    "- *Ablation* studies are often conducted\n",
    "    - Replace an innovative choice with the standard; compare before/after metrics\n",
    "    \n",
    "The authors conduct a systematic study in an attempt to determine\n",
    "- which choices are best\n",
    "- create a model (T5) inspired by the best choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Enablers\n",
    "\n",
    "## Text to Text as Universal NLP API\n",
    "\n",
    "A key enabler of the study  was the formulation of a Universal NLP API: Text to Text [which we studied](NLP_Text_to_Text.ipynb).\n",
    "\n",
    "All NLP tasks can be re-expressed\n",
    "- Translating (potentially structured) Input Text to flat sequences of Processed Text\n",
    "- Outputting flat sequences of Text.\n",
    "\n",
    "In particular: both the pre-training and fine tuning use the Text to Text format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Colossal Clean Crawled Corpus (C4 ) training dataset\n",
    "\n",
    "[Common Crawl](https://commoncrawl.org/) is consists of data obtained via *web-scraping* over many years\n",
    "- Large: 20TB **per month**\n",
    "- Lacking quality for NLP tasks\n",
    "    - not Natural Language: source code, menus, error messages\n",
    "    - offensive content\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In response to this, the authors created Colossal Clean Crawled Corpus (C4), a cleansed version of Common Crawl\n",
    "- retain only sentences\n",
    "    - ending in proper NL punctuation (e.g., period, exclamation)\n",
    "    - with at least 3 words\n",
    "    - without indicators of code, e.g, `!javascript`\n",
    "- **de-duplicated**\n",
    "- filter out \"bad words\"\n",
    "- filter out **non-English** pages\n",
    "    - since most tasks in set of benchmarks used were English-only\n",
    "    \n",
    "See the paper's [section](https://arxiv.org/pdf/1910.10683.pdf#page=5) for full details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Downstream tasks\n",
    "\n",
    "The research uses a predetermined set of benchmarks that measure the performance on *downstream* tasks such as\n",
    "- Sentence acceptability\n",
    "- Sentiment analysis\n",
    "- Sentence similarity\n",
    "- Sentence completion\n",
    "- Question answering\n",
    "- Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Choices examined\n",
    "\n",
    "## Architecture\n",
    "\n",
    "All models studied used the Transformer architecture\n",
    "- dominates simpler RNN, LSTM alternatives\n",
    "- by facilitating long range dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The original Transformer architecture consists of an Encoder and Decoder\n",
    "- Encoder creates alternate representation of input\n",
    "    - Ordinary self-attention (not causal) to the (layer's) inputs\n",
    "- Decoder acts auto-regressively to create output\n",
    "    - Uses masked attention to enforce causal ordering\n",
    "        - can only attend to past outputs\n",
    "        - can't \"peek ahead\" (during training) into output that will only be created in a future step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Architecture choices**\n",
    "\n",
    "Encoder/Decoder\n",
    "\n",
    "Encoder only\n",
    "- BERT is an example of an Encoder only architecture\n",
    "\n",
    "Decoder only\n",
    "- typically used for Language modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Attention choices\n",
    "\n",
    "The basis for the Transformer is the *Attention* mechanism.\n",
    "\n",
    "There are different \"flavors\" of Attention.\n",
    "- restricts which parts of the input/output may be *attended to* (accessed) by the model at each time step $\\tt$\n",
    "- where output $\\y_\\tp$ is generated at time step $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider a task with\n",
    "- input sequence $\\x_{(1)}, \\ldots, \\x_{(\\bar T)}$\n",
    "- output sequence $\\y_{(1)}, \\ldots, \\y_{(T)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Fully visible* attention\n",
    "- usually refers to which parts of the input $\\x$ may be attended to by the Encoder\n",
    "- everything: $\\x_{(1)}, \\ldots, \\x_{(\\bar T)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Causal attention* \n",
    "- usually refers to which parts of the output $\\y$ may be attended to by the Decoder\n",
    "- only those parts already generated $\\y_{(1)}, \\ldots \\y_{(\\tt-1)}$\n",
    "    - can't peek into the future\n",
    "        - at training time, the *complete* target is available, and hence, the future could be visible\n",
    "        - impossible at test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Text to Text encoding of a task, the input and output sequences\n",
    "- are usually concatenated into a single sequence: the *Processed Input*\n",
    "- with separator tokens \n",
    "\n",
    "\n",
    "Let concatenated sequence $\\dot\\x_{(1)}, \\ldots, \\dot\\x_{(\\bar T)+T}$ be defined\n",
    "$$\\dot\\x_\\tp =\n",
    "\\begin{cases}\n",
    "\\x_\\tp & \\tt \\le \\bar T \\\\\n",
    "\\y_{(\\tt - \\bar T)} & \\tt \\gt \\bar T\n",
    "\\end{cases}\n",
    "$$\n",
    "where we are ignoring the separator tokens for simplicity of notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need to re-state the definition of the flavors of attention\n",
    "- relative to the Processed Input (concatenated sequence)\n",
    "- without reference to the Encoder or Decoder part\n",
    "    - e.g., there is a Decoder-only architecture possible, which can also attend to $\\x$\n",
    "        - which is part of the Processed Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Attention choices**\n",
    "\n",
    "*Fully visible* attention: \n",
    "- everything is visible: $\\dot\\x_{(1)}, \\ldots, \\dot\\x_{(\\bar T)+T}$\n",
    "\n",
    "*Causal* attention: $1 \\le \\tt' \\lt \\tt$\n",
    "- only parts prior to the current output are visible: $\\dot\\x_{(1)}, \\ldots, \\dot\\x_{(\\tt-1)}$\n",
    "    \n",
    "*Prefix Language Model (LM)* attention\n",
    "- the input $\\x$ is fully visible; the only part of $\\y$ visible is that which has been generated prior to $\\tt$\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dot\\x_{(1)}, \\ldots, \\dot\\x_{\\bar T}  & \\text{visible at } \\tt \\le \\bar T & \\text{input fully visible}\\\\\n",
    "\\dot\\x_{(1)}, \\ldots, \\dot\\x_{(\\tt-1)} & \\text{visible at } \\tt \\gt \\bar T & \\text{inputs, and outputs that have been generated before } \\tt \\text{, are visible}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model size\n",
    "\n",
    "A Transformer model can be characterized by a number of parameters\n",
    "- Number of layers\n",
    "    - number of Transformer blocks in the Encoder or Decoder stacks\n",
    "- $d_\\text{head},  d_\\text{kv}$\n",
    "    - size of each head\n",
    "    - corresponds to the size of the keys and values in the Attention Lookup\n",
    "- $n_\\text{heads}$\n",
    "    - number of heads\n",
    "- $d_\\text{model} = n_\\text{heads} * d_\\text{head}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised objective\n",
    "\n",
    "**Unsupervised objective choices**\n",
    "\n",
    "Language Modeling (LM)\n",
    "- predict the next token\n",
    "\n",
    "Masked Language Modeling (MLM)\n",
    "- a *denoising* objective\n",
    "- corrects corrupted (*masked*) inputs\n",
    "    - 15% of input tokens corrupted, i.e., replaced by\n",
    "        - random token, 10% of the time\n",
    "        - missing token `[MASK]`, 90% of the time\n",
    "\n",
    "n.b., MLM claimed to the \"standard\" because it has been observed to perform better than LM (predict the next) objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A less familiar objective is *deshuffling*\n",
    "- input: re-ordered tokens\n",
    "- output: tokens in correct order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "**Unsupervised Pre-training Dataset choices**\n",
    "\n",
    "C4\n",
    "\n",
    "Unfiltered C4\n",
    "- remove filtering for non-text, bad words\n",
    "- retain English-only filter\n",
    "\n",
    "RealNews-like C4\n",
    "- C4 limited to news websites used for the RealNews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Wikipedia C4\n",
    "\n",
    "Wikipedia + Toronto Book Corpus (CBC)\n",
    "- books (ebooks) are different stylistically from Wikipedia articles\n",
    "\n",
    "WebText-like C4\n",
    "- WebText was created for GPT\n",
    "- filtered to pages ranked with high human scores on Reddit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Size of training dataset\n",
    "\n",
    "Varying total number of tokens used in pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Strategy\n",
    "\n",
    "\n",
    "### Downstream task: Fine tuning choices\n",
    "\n",
    "There are several variants of the Fine-Tuning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our original presentation on Transfer Learning\n",
    "- we grafted a Target-task specific Classification head\n",
    "- on to a prefix of the pre-trained architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have the choice of which parameters to adjust during fine-tuning:\n",
    "- just the parameters in the newly grafted (uninitialized) Head\n",
    "    - *freezing* the parameters of the base model\n",
    "    - to possible corruption of base model parameters\n",
    "        - due to initial large gradients caused by untrained Head\n",
    "- *Gradual unfreezing* of the base parameters\n",
    "    - freeze parameters of deep layers of base until Head parameters have been somewhat trained\n",
    "    - gradually unfreeze parameters of earlier layers as deeper layer parameters have adapted\n",
    "- all the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This doesn't directly apply to the Text to Text format\n",
    "- no Classification Head\n",
    "- entire Decoder needs to be trained to produce the Target Task sequence\n",
    "\n",
    "Instead, the *freezing* strategy used is to insert *adapter layers*\n",
    "- Dense-ReLu-Dense blocks inserted after the Feed Forward (FF) final component of the Transformer block\n",
    "    - for each layer\n",
    "- number of inputs and outputs are the same\n",
    "- so can be inserted without affecting rest of the architecture\n",
    "- comparable to training only the Classifier Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The analog of gradual unfreezing for the Transformer is to allow\n",
    "- gradual unfreezing of parallel layers in the Encoder and Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Upstream task: Include Multi Task Learning as part of Pre-Training ?\n",
    "\n",
    "Rather than *only* using Unsupervised Pre-Training upstream (i.e., before Fine Tuning)\n",
    "- we could try Multi Task Learning\n",
    "- training set a mixture of examples from task-specific training datasets\n",
    "- several variants of determining the proportions in the mixture\n",
    "\n",
    "Note that Unsupervised Pre Training **is one of the tasks included in the mixture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Baseline\n",
    "\n",
    "A baseline model is fixed by making a set of choices.\n",
    "\n",
    "Experiments are conducted by varying choices one at a time.\n",
    "\n",
    "Here are the choices for the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Architecture**\n",
    "\n",
    "Encoder/Decoder Transformer\n",
    "- similar to BERT base model\n",
    "    - 12 layers\n",
    "    - $d_\\text{model} = n_\\text{heads} * d_\\text{head} = 768$\n",
    "        - $n_\\text{heads} = 12, d_\\text{head} = d_\\text{kv} = 64$\n",
    "        \n",
    "- Cannot completely match Encoder only models\n",
    "    - so constructed model is \"similar\" but not identical for some models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Training**\n",
    "\n",
    "Maximum Likelihood: Teacher forcing, Cross-Entropy Loss\n",
    "\n",
    "Maximum input sequence length = 512\n",
    "\n",
    "Tokens per batch = Max input sequence length * batch size = $2^{9} * 2^{7} = 2^{16} = 65,536$\n",
    "\n",
    "Number of pre-training steps = $2^{19} = 524,288$\n",
    "\n",
    "Number of tokens in pre-training = Tokens per batch * Number of pre-training steps = $2^{16} * 2^{19} = 2^{35}$\n",
    "\n",
    "Number of fine tuning steps = $2^{18}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Vocabulary**\n",
    "- SentencePiece with 32K pieces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Unsupervised Training Objective**\n",
    "\n",
    "Masked Language Modeling (MLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "[Paper: reflections](https://arxiv.org/pdf/1910.10683.pdf#page=41)\n",
    "\n",
    "## Text to Text\n",
    "\n",
    "Comparable performance to \"native\" model\n",
    "\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Encoder/Decoder\n",
    "\n",
    "Parameter sharing (between Layers): didn't hurt\n",
    "\n",
    "Reducing number of layers hurts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised objective\n",
    "\n",
    "MLM\n",
    "\n",
    "- Little difference in masking schemes\n",
    "- MLM corruption rate: unimportant, up to 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Subsets of C4 limited to domain-specific examples\n",
    "- performs better for a *few* downstream tasks\n",
    "- **conditional** on the narrower datasets not being too small\n",
    "    - which results in repeated examples when training with a minimum number of tokens constraint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training strategy\n",
    "\n",
    "- Updating *all* parameters during Fine Tuning worked best\n",
    "- Including Multi Task Learning was **not** beneficial\n",
    "    - i.e., could not find a mixing strategy that outperformed Unsupervised Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Keras fine-tuning, transfer learning](https://keras.io/guides/transfer_learning/)\n",
    "    - Transfer Learning vs Fine tuning\n",
    "    - Recursive setting of the trainable attribute\n",
    "    - Workflow\n",
    "    \n",
    "- [Documentation: DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
    " - [DistilBert code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L24)\n",
    "- [DistilBert model card](https://huggingface.co/distilbert-base-uncased)\n",
    "- [BERT model card](https://huggingface.co/bert-base-uncased#limitations-and-bias)\n",
    "- [Documentation: Models](https://huggingface.co/docs/transformers/main_classes/model)\n",
    "- [Fine tuning lesson](https://huggingface.co/docs/transformers/training)\n",
    "    - [Fine tune for downstream tasks](https://huggingface.co/docs/transformers/tasks/sequence_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Behind the pipeline](https://huggingface.co/course/chapter2/2?fw=tf)\n",
    "\n",
    "- [Course Chapt 2, putting it all together: Using a pretrained model for sequence classification](https://huggingface.co/course/chapter2/6?fw=tf)\n",
    "- [Using a Model: Chapt 2](https://huggingface.co/course/chapter2/3?fw=tf)\n",
    "\n",
    "[My notebook: Finetuning FinancialPhraseBank, based on the Fine_tune_Hugg..](HuggingFace/Course/Financial_PhraseBank.ipynb)\n",
    "- [My Fine_tune_Hugg.. from course, can't find in course anymore](HuggingFace/Course/Fine_tune_HuggingFace_model_in_Keras_with_plain_datasets.ipynb)\n",
    "- seems to have changed\n",
    "    - using Yelp rather than IMdB\n",
    "        - [imbdb is in transformers/doc, but now uses Trainer](https://huggingface.co/docs/transformers/tasks/sequence_classification)\n",
    "    - using Trainer\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hugging Face\n",
    "\n",
    "[Hugging Face](huggingface.co) is a Data Science platform that (among many other functions) hosts\n",
    "- Pre-trained models\n",
    "- Datasets\n",
    "\n",
    "They are particularly (but not exclusively) known from Transformers and NLP tasks.\n",
    "\n",
    "We will use them as our paradigm for Transfer Learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our overview will necessarily be brief.\n",
    "\n",
    "We recommend the [free, on-line course](https://huggingface.co/course)\n",
    "- We will not explain the datasets API\n",
    "    - similar to Tensorflow Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: more than just a model\n",
    "\n",
    "You will find many models for NLP on HuggingFace.\n",
    "\n",
    "One of the most important aspects of a Model Hub for NLP is\n",
    "- providing a consistent interface to multiple models\n",
    "- with different authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre and post processing\n",
    "\n",
    "To ensure universality of models obtained from many sources\n",
    "- all NLP models take sequences of *tokens* as inputs\n",
    "- **not** sequences of words (the raw input format)\n",
    "\n",
    "Thus, there needs to be some *pre-processing* performed before using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, there may need to be some *post-processing* performed on the model output.\n",
    "\n",
    "For example: creating a task-specific \"head\"\n",
    "-  e.g., classifier over *our task's* classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition to models, HuggingFace\n",
    "- supplies common pre-processors and post-processors\n",
    "- wraps them all up into a simple to use, powerful organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One model, many flavors\n",
    "\n",
    "As we shall see, HuggingFace models come in many variations\n",
    "- `Keras` or `Pytorch`\n",
    "- with a \"head\" specific to a common task\n",
    "\n",
    "This reduces the need for user customization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HuggingFace also makes it easy to match the correct pre-processor (Tokenizer) to a model\n",
    "- a model is identified by a \"check-point\" referring to its training dataset\n",
    "\n",
    "        checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "- one instantiates a model from a checkpoint\n",
    "\n",
    "         model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "    - this model has a \"head\" specialized for the task of Sequence Classification (e.g., Sentiment Analysis)\n",
    "        - the `ForSequenceClassification` suffix\n",
    "    - this model is implemented in TensorFlow\n",
    "        - the `TF` prefix\n",
    "- it's easy to choose the \"correct\" Tokenizer by using the same one as was used in training the model\n",
    "      \n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing input examples\n",
    "\n",
    "A unique aspect to sequence inputs is that the sequence length varies across examples.\n",
    "\n",
    "Hence, \"padding\" is usually necessary to make example lengths uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is tempting equalize the length of each example to the length of the longest example in the *training dataset*.\n",
    "\n",
    "This is not the best strategy\n",
    "- the model works in much smaller *mini-batches*\n",
    "- only necessary to equalize the length of each example *within* a batch\n",
    "    - not *across* all batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Equalizing sequence length of examples within a batch is just one example of\n",
    "preparing examples in a batch.\n",
    "\n",
    "Another case: masking words (usually randomly) in a example for a Masked Language Modeling task.\n",
    "\n",
    "HuggingFace has an abstract class [`DataCollator`](https://huggingface.co/docs/transformers/main_classes/data_collator#data-collator) to prepare examples within a batch\n",
    "- specializations for common tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Putting it all together: running an NLP task\n",
    "\n",
    "Easy !  So is actually running all the steps needed to generate output\n",
    "\n",
    "    sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "    \n",
    "    tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    \n",
    "    output = model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And one \"not so easy\" part\n",
    "- Like any classifier: the output is a *vector* of probabilities over the universe of tokens\n",
    "    - Technically: they are \"logits\" rather than probabilities\n",
    "- So the *user* needs to sample from this vector in order to choose a single output token\n",
    "    - e.g., the one with highest probability\n",
    "- This maximizes the flexibility for the user\n",
    "    - to sample rather than choose the token with highest probability\n",
    "    - to add further logic\n",
    "        - sample \"top 5\" tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Using a pre-trained model for NLP (w/o Transfer Learning)\n",
    "\n",
    "Our first demo will be to show how to use a pre-trained model.\n",
    "- We will not (yet) specialize it to a new Task\n",
    "\n",
    "The task is Text Sequence Classification (sentiment)\n",
    "\n",
    "The model is based on the Transformer architecture.\n",
    "\n",
    "We give a quick review of how Transformers are typically used for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pre processing**\n",
    "\n",
    "Text is a sequence of words, but the words must be parsed into *tokens*\n",
    "- a word may turn into multiple tokens: Byte Piece Encoding\n",
    "\n",
    "The tokens must be converted into integer indices in a finite vocabulary\n",
    "- It is the sequence of *token ids* that is consumed by the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Post processsing**\n",
    "\n",
    "Hugging Face Classification models are slightly different in that they return\n",
    "- logits (scores)\n",
    "- **not** probabilities\n",
    "\n",
    "We can use `argmax` on the score to decide the class, but it's nice to see the associated probability too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's go to the [notebook](HF_quick_intro_to_models.ipynb)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A model has many \"flavors\"\n",
    "\n",
    "In our discussion of Modern NLP, we highlighted the \"Text to Text Universal API\"\n",
    "- Can translate many NLP tasks into variations of translating Text to Text\n",
    "- A post-processing step can map back from the output text to the actual Task outputs\n",
    "\n",
    "Hugging Face\n",
    "- implements a \"base model\" (universal API)\n",
    "- and the derives specializations with task-specific \"heads\"\n",
    "\n",
    "Moreover, Hugging Face models usually have implementations in both TensorFlow and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The name of a Hugging Face model is a compound string indicating\n",
    "- the base\n",
    "- the specialized head\n",
    "- the implementation language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Illustrating this with the DistilBERT model implemented in TensorFlow ([github](https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L24)), here is the class hierarchy:\n",
    "- `class TFDistilBertPreTrainedModel(TFPreTrainedModel):`\n",
    "- `class TFDistilBertModel(TFDistilBertPreTrainedModel)`\n",
    "- `class TFDistilBertForMaskedLM(TFDistilBertPreTrainedModel, TFMaskedLanguageModelingLoss):`\n",
    "- `class TFDistilBertForSequenceClassification(TFDistilBertPreTrainedModel, TFSequenceClassificationLoss):`\n",
    "- `class TFDistilBertForTokenClassification(TFDistilBertPreTrainedModel, TFTokenClassificationLoss):`\n",
    "- $\\vdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `TF` prefix indicates that the implementation language is TensorFlow.\n",
    "\n",
    "- There is an abstract class for Pretrained TensorFlow models: `TFPreTrainedModel)`\n",
    "- `TFDistilBertPreTrainedModel` is a pretrained model\n",
    "- There are specialization of `TFDistilBertPreTrainedModel` for each NLP task\n",
    "    - `TFDistilBertForMaskedLM` for the Masked Language Modelin task\n",
    "    - `TFDistilBertForSequenceClassification` for the Sequence Classification task\n",
    "    - `TFDistilBertForTokenClassification` for the Token Classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You get to choose !\n",
    "- If you choose a model instance with the `*For*` pattern in the name\n",
    "    - you get a pre-trained model with a Classification head (post-processing) for a specific task\n",
    "- If you choose a model instances *without* the `*For*` pattern\n",
    "    - you get a model without a head (or post-processing)\n",
    "    - you can add your own\n",
    "    \n",
    "You can also instantiate a model *without* pre-trained weights\n",
    "- just the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case study:`TFDistilBertForSequenceClassification`\n",
    "\n",
    "Let us examine the implemenation of the `TFDistilBert` specialization for Sequence Classification.\n",
    "\n",
    "[github link to class `TFDistilBertForSequenceClassification`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L699)\n",
    "\n",
    "`class TFDistilBertForSequenceClassification`\n",
    "- Derives from `TFDistilBertPreTrainedModel`\n",
    "- creates a Classifier head \n",
    "- modifies the `call` method\n",
    "    - pass the input sequence to the base `DistilBert` model\n",
    "    - the `Distilbert` model output is\n",
    "        - sequence of latent representations of length 768 (one latent per element of the input sequence)\n",
    "    - choose the single sequence element at position 0, corresponding to the the special `[CLS]` input token\n",
    "    - pass the single element to a two layer Classifier head\n",
    "        - `preclassifier`, 'classifier`\n",
    "        - `Dense` layers separated by `Dropout`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Part of a good model hub is a *model card* for each model that describes\n",
    "- The model\n",
    "- The data on which it was trained\n",
    "- Gives reference to the original work (paper, code)\n",
    "- Discusses potential *bias*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bias of models (really, the data on which they were trained) is important and is being increasingly recognized.\n",
    "\n",
    "Models are often trained from publicly available Web content (e.g., Wikipedia).\n",
    "\n",
    "Although the datasets were not designed to be biased\n",
    "- the frequency of biased concepts that they contain\n",
    "- causes the biases to be transfered to models on which they are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Intro course, we discussed how the GloVe word embeddings may reveal bias:\n",
    "\n",
    "$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{doctor - man + woman} &  \\approx_{n',d} & \\text{nurse } \\\\\n",
    "\\text{mechanic  - man + woman} &  \\approx_{n',d} & \\text{teacher } \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Discovering and documenting potential bias is non-trivial and potentially time-consuming.\n",
    "\n",
    "[Here are the biases](https://huggingface.co/distilbert-base-uncased#limitations-and-bias) discussed on the model card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Here](https://huggingface.co/distilbert-base-uncased) is the model card for `DistilBert`.\n",
    "\n",
    "For the purposes of Transfer Learning:\n",
    "- note that the implemention of `DistilBert` on Hugging Face [consumed 90 hours on 8 GPUs](https://huggingface.co/distilbert-base-uncased#pretraining)\n",
    "- we will leverage this work without any training cost to us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning + Fine-Tuning: rolling our own specialization of `DistilBert`\n",
    "\n",
    "We will now demonstrate\n",
    "- Transfer Learning\n",
    "    - grafting a Classification head onto a pre-trained `DistilBert` model\n",
    "- Fine-tuning\n",
    "    - once our grafted model Classifier head is trained\n",
    "        - train *all* the weights, including the contained `DistilBert` model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This module is a nice summary of the course\n",
    "- We use a `Functional` architecture\n",
    "- Override the `call` method\n",
    "- Transfer Learning + Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are going to adapt `DistilBert` to a new \"Target\" task: Text Sequence Classification\n",
    "- `DistilBert` was trained on the Masked Language Modelling task\n",
    "- Our task is different: Text Sequence Classification\n",
    "- We will therefore invoke a \"headless\" version of the model and graft on our own head\n",
    "  - which will need to be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Highlights**\n",
    "- Create a new model that *contains* a pre-trained `DistilBert`\n",
    "- Graft on a Classifier head\n",
    "    - override `call` method\n",
    "        - invoke the contained `DistilBertModel`\n",
    "        - post-process\n",
    "        - pass result to Classifier head\n",
    "- Transfer Learning\n",
    "    - freeze weights of the contained `DistilBert`\n",
    "    - train weights of Classifier head\n",
    "- Fine Tuning\n",
    "    - re-train *all* the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visit the [notebook](Transfer_Learning_example_HF.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

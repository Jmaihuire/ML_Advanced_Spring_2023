{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-task learning\n",
    "\n",
    "One area of recent interesting is *multi-task learning*\n",
    "- Training a model to implement multiple tasks\n",
    "\n",
    "A model that implements a single task computes\n",
    "$$\\pr{\\text{output | input}}$$\n",
    "\n",
    "A model that implements several tasks computes\n",
    "$$\\pr{\\text{output | input, task-id }} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When training a model for multiple tasks, the training examples would look something like:\n",
    "$$\\begin{array}[lll] \\\\\n",
    "(  \\mathsf{Translate \\; to \\;French} , & \\text{English text} ,  & & \\text{French Text}) \\\\\n",
    "( \\mathsf{Answer \\; the \\; question} , & \\text{document} , & \\text{question} , & \\text{answer}) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Text is almost a universal encoding so NLP is a natural way of expressing multiple tasks.\n",
    "\n",
    "So a natural extensions of a Language Model is to solve multiple tasks\n",
    "- Encode your specific task as an input that can be handled by a Language Model\n",
    "- That's one advantage of Byte Pair Encoding\n",
    "    - No special per-task pre-processing needed for a task's training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will take the idea of Multi-task learning one step further\n",
    "- Learning how to solve a task **without** explicitly training a model !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pre-train, prompt, predict \n",
    "\n",
    "We have presented the \"Unsupervised Pre-Training + Supervised Fine-Tuning\" paradigm.\n",
    "\n",
    "Considering that\n",
    "- Language models seen to learn universal, task-independent language representation\n",
    "- Text-to-text is a universal API for NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can raise the question\n",
    "- Is Supervised Fine-Tuning even necessary ?\n",
    "- Can a Language Model learn\n",
    "    - to solve a task that it has not been explicitly trained on\n",
    "    - by \"learning\" the task at *test time*\n",
    "        - no parameter updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are some  practical impediments to answering this question\n",
    "- How does the LM model \"understand\" that it is being asked to solve a particular task ?\n",
    "- How does the LM model \"understand\" the input-output relationship involved in the new task ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The solution to both impediments is to *condition the LLM* by pre-pending *exemplars*  (*demonstrations*)\n",
    "at the start of every query\n",
    "- examples providing \"context\" in which to evaluate future prompts\n",
    "- paradigmatic examples demonstrating a prompt and desired response\n",
    "\n",
    "We will call this the *pre-prompt* or *context*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, we can describe Translation between languages with the following pre-prompt\n",
    "\n",
    "    Translate English to French\n",
    "    \n",
    "    sea otter =>  loutre de mer\n",
    "    \n",
    "    peppermint => menthe poivree\n",
    "    \n",
    "    plush giraffe => girafe peluche\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The pre-prompt consists of\n",
    "- an initial string describing the task: \"Translate English to French\"\n",
    "- a number of examples\n",
    "    - English input, French output, Separated by a `=>`\n",
    "    \n",
    "The expectation is that when the user presents the prompt\n",
    "\n",
    "         cheese => \n",
    "         \n",
    "the model will respond with the French translation of `cheese`.\n",
    "- the \"next words\" predicted by the Language Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the exemplars are given at *inference* time **not** training time\n",
    "- the model's weights are **not updated**\n",
    "- the examplars only condition the model into generating specific output\n",
    "\n",
    "This paradigm has been called [\"Pre-train, Prompt, Predict\"](https://arxiv.org/pdf/2107.13586.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More formally: \n",
    "- Let $C$ (\"context\") denote the pre-prompt.\n",
    "- Let $\\x$ denote the \"query\" (e.g., `cheese =>`)\n",
    "\n",
    "The unconditional Language Modeling objective\n",
    "$$\n",
    "\\pr{\\y | \\x}\n",
    "$$\n",
    "is to create the sequence $\\y$ that follows the sequence of prompt $\\x$.\n",
    "\n",
    "Here, the pre-prompt conditions the model's objective\n",
    "$$\n",
    "\\pr{\\y | \\x,  C}\n",
    "$$\n",
    "to create the sequence $\\y$ that follows from the exemplars $C$ and prompt $\\x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To turn this into a Language Modeling task using the Universal API\n",
    "- we need to represent the exemplars and the prompt \n",
    "as a sequence.\n",
    "\n",
    "We create the sequence $\\dot \\x$\n",
    "by concatenating\n",
    "\n",
    "- some number $k$ of exemplars: $\\langle \\x^{(1)}, \\y^{(1)} \\rangle, \\ldots, \\langle \\x^{(k)}, \\y^{(k)} \\rangle $\n",
    "- the prompt string $\\x$\n",
    "- delimiting elements by separator characters $\\langle \\text{SEP}_1 \\rangle. \\langle \\text{SEP}_2 \\rangle$\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\dot \\x = \\text{concat} (  & \\x^{(1)}, \\langle \\text{SEP}_1 \\rangle, \\y^{(1)}, \\langle \\text{SEP}_2 \\rangle,  \\\\\n",
    "              &   \\vdots \\\\\n",
    "              &   \\x^{(k)}, \\langle \\text{SEP}_1 \\rangle, \\y^{(k)}, \\langle \\text{SEP}_2 \\rangle, \\\\\n",
    "              &   \\x \\\\\n",
    "              & ) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The LLM then computes\n",
    "$$\n",
    "\\pr{ \\y | \\dot \\x }\n",
    "$$\n",
    "\n",
    "For convenience, we will just write this as the conditional probability\n",
    "$$\n",
    "\\pr{\\y | \\x,  C}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Zero shot learning: learning to learn\n",
    "\n",
    "Let $k$ denote the number of exemplars in the pre-prompt.\n",
    "\n",
    "We can ask how well a LLM performs on an unknown query with varying size of $k$.\n",
    "\n",
    "- **Few shot learning**: $10 \\le k \\le 100$ typically\n",
    "- **One shot learning**: $k = 1$\n",
    "- **Zero shot learning** $k=0$\n",
    "\n",
    "A picture will help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Few/One/Zero shot learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_Few_Shot_Training.png\"\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is this even possible ?!   Learning a new task with **zero** exemplars ?\n",
    "\n",
    "Let's look at the reported results from the third generation GPT-3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Few/One/Zero shot learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_Few_Shot_Accuracy.png\"\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Prompt engineering\n",
    "\n",
    "You see how the behavior of the LLM can be affected by the exact form of the prompt.\n",
    "\n",
    "There is a whole literature on creating successful prompts: [Prompt engineering](https://arxiv.org/pdf/2107.13586.pdf), [Chain of thought prompting](https://arxiv.org/pdf/2201.11903.pdf)\n",
    "- Providing enough context to condition the model to \"understand\"\n",
    "    - That \"Translate English to French\" relates to some examples seen (implicitly) in training\n",
    "    - and that the string `=>` suggests a relationship between the input and output\n",
    "        - perhaps generalizing examples seen in training\n",
    "\n",
    "\n",
    "OpenAI provides [helpful examples](https://platform.openai.com/examples) for prompting.\n",
    "\n",
    "[See Appendix G](https://arxiv.org/pdf/2005.14165.pdf#page=51) (pages 50+) for examples of prompts for many other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Chain of thought prompting\n",
    "\n",
    "[Paper: Chain of thought prompting](https://arxiv.org/pdf/2201.11903.pdf)\n",
    "\n",
    "In school, students are often tasked with solving problems involving multiple steps.\n",
    "\n",
    "LLM's are better at multi-step reasoning tasks when they have been conditioned to answer step by step.\n",
    "\n",
    "We call this *chain of thought (CoT)* prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The exemplars used in CoT prompting\n",
    "- demonstrate step by step reasoning in the expected output\n",
    "\n",
    "We can see the difference in the exemplar's \"Example Output\" section\n",
    "- using \"Standard Prompting\" (on the left)\n",
    "- versus using \"CoT Prompting\" (on the right)\n",
    "\n",
    "<img src=\"images/cot_prompt_example.png\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does this apply to the case of *zero* exemplars (zero-shot learning) ?\n",
    "\n",
    "It turns out that step by step reasoning can be elicited\n",
    "- Just by adding the phrase [\"Let's think step by step\"](https://arxiv.org/pdf/2205.11916.pdf) to the end of the query\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's ask ChatGPT to solve a multi-step reasoning problem in a zero-shot setting.\n",
    "\n",
    "As you can see: it comes close, by produces an incorrect answer.\n",
    "\n",
    "<img src=\"images/cot_prompt_no_step_by_step.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's run the same query but append a request to answer step-by-step.\n",
    "\n",
    "<img src=\"images/cot_prompt_step_by_step.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using zero-shot to create new applications\n",
    "\n",
    "With a little cleverness, one can almost trivially create a new application using a LLM in zero-shot mode\n",
    "- create the prefix of a prompt describing the task\n",
    "- append the user input to the prefix to complete the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we use [`ChatGPT`](https://chat.openai.com/chat) to create an app that summarizes a conversation\n",
    "- we create a prompt with a \"place-holder\" (in braces `{..}`) for user input\n",
    "\n",
    "`prompt = Summarize the following conversation: {user input}`\n",
    "\n",
    "<img src=\"images/chatgpt_summarize_conversation_example.png\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we use ChatGPT as a programming assistant\n",
    "\n",
    "`prompt = Write a Python function that does the following: {task description}`\n",
    "\n",
    "<img src=\"images/chatgpt_program_generation_example.png\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some more, creative examples\n",
    "- [Spreadsheet add-in to perform lookups](https://twitter.com/pavtalk/status/1285410751092416513)\n",
    "- [Generate a web page from a description](https://twitter.com/sharifshameem/status/1283322990625607681)\n",
    "\n",
    "References found in: http://ai.stanford.edu/blog/understanding-incontext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How is zero-shot learning possible ? Some theories\n",
    "\n",
    "**Theory 1**\n",
    "\n",
    "- The training set contains explicit instances of these out of sample tasks\n",
    "\n",
    "**Theory 2**\n",
    "\n",
    "- The super-large training sets  contain *implicit* instances of these out of sample tasks\n",
    "    - For example: an English-language article quoting a French speaker in French with English translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One thing that jumps out from the graph:\n",
    "- Bigger models are more likely to exhibit meta-learning\n",
    "\n",
    "**Theory 3**\n",
    "\n",
    "The training sets are so big that the model \"learns\" to create groups of examples with a common theme\n",
    "- Even with the large number of parameters, the model capacity does not suffice for example memorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another thing to consider\n",
    "- The behavior of an RNN depends on *all* previous inputs\n",
    "    - It has memory (latent state, etc.)\n",
    "    \n",
    "So Few Shot Learning may work by \"priming\" the memory with parameters for a specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Social concerns\n",
    "\n",
    "The team behind GPT is very concerned about potential misuse of Language Models.\n",
    "\n",
    "To illustrate, they conducted an experiment in having a Language Model construct news articles\n",
    "- Select title/subtitle of a genuine news article\n",
    "- Have the Language Model complete the article from the title/subtitle\n",
    "- Show humans the genuine and generated articles and ask them to judge whether the article was written by a human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Human accuracy in detecting model generated news articles</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/LM_GPT_model_generated_news.png\" width=80%></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Picture from: https://arxiv.org/pdf/2005.14165.pdf</center></td>\n",
    "    </tr>   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The bars show the range of accuracy across the 80 human judges.\n",
    "\n",
    "- 86% accuracy detecting articles created by a really bad model (the control)\n",
    "- 50% accuracy detecting articles created by the biggest models\n",
    "\n",
    "It seems that humans might have difficulty distinguishing between genuine and generated articles.\n",
    "\n",
    "The fear is that Language Models can be used\n",
    "- to mislead\n",
    "- to create offensive speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "Differential Privacy\n",
    "- [Deep Learning with Differential Privacy](https://arxiv.org/pdf/1607.00133.pdf)\n",
    "    - important paper\n",
    "- [DP-SGD explained](https://mukulrathi.com/privacy-preserving-machine-learning/deep-learning-differential-privacy/)\n",
    "    - good article\n",
    "    \n",
    "Leaking training examples\n",
    "- [Extracting Training Data from LLM](https://arxiv.org/pdf/2012.07805.pdf)\n",
    "\n",
    "Implementing Privacy\n",
    "- [TensorFlow Privacy](https://www.tensorflow.org/responsible_ai/privacy/tutorials/classification_privacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training a model with sensitive information\n",
    "\n",
    "A key ingredient in the ability of a Machine Learning model to solve a task is the quantity and quality\n",
    "of training examples.\n",
    "\n",
    "Each person has \"sensitive\" data (e.g., health, income) that\n",
    "- might be valuable for training a ML model\n",
    "- but whose \"leaking\" (public disclosure) would be harmful\n",
    "\n",
    "The potential for harm implies that\n",
    "- if an individual's sensitive data is to be used for training a ML model\n",
    "- there needs to be a guarantee that an individual's sensitive data will not be disclosed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Beyond the potential for causing harm, removing the possibility of disclosure has benefit.\n",
    "\n",
    "Imagine the potential for medical advancement\n",
    "- if we were able to contribute our medical, genealogical, social data\n",
    "- as examples for training models to predict likelihood\n",
    "    - of disease\n",
    "    - success of a treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Extracting Training Data from LLM](https://arxiv.org/pdf/2012.07805.pdf)\n",
    "\n",
    "We first ask: do Large Language Models (LLM) leak training examples ?\n",
    "\n",
    "Memorization of training examples (a prerequisite to the possibility of leaking) is often thought of as a consequence of model overfitting.\n",
    "\n",
    "As the number of weights of an LLM increases, the potential for memorization would seem to increase.\n",
    "\n",
    "On the other hand, overfitting is also associated with seeing the same example in many epochs of training.\n",
    "- But LLM training is on very few (perhaps a single) epoch\n",
    "- The datasets are so big, that even one epoch provides many gradient updates (one per mini-batch)\n",
    "    - Reducing the need for multiple epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The paper tries to quantify how much training data leaks from the GPT-2 model\n",
    "- The training data comes from public sources that are identified\n",
    "    - mitigates the harm of the authors revealing leaked data\n",
    "- But the exact training data is *not* open-source\n",
    "    - can't verify a suspected leak against actual training examples\n",
    "    - but a Web search can confirm the source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Methodology\n",
    "\n",
    "The authors propose a number of techniques to extract potential training examples.\n",
    "\n",
    "They are all based on using \n",
    "- the LLM's autoregressive behavior\n",
    "- to generate \"candidate\" sequences of text\n",
    "- evaluating whether the candidate sequence is a training example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the autogregressive LLM computes\n",
    "$$\n",
    "\\pr{\\y_\\tp | \\y_{([1:\\tt-1]) } }\n",
    "$$\n",
    "\n",
    "That is, it \n",
    "- creates a probability distribution $\\pr{\\y_\\tp}$ of the next (at position $\\tt$) token\n",
    "- conditional on all prior outputs $\\y_{([1:\\tt-1])}$\n",
    "    - we are including the \"seed\" sequence in $\\y$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The actual token $\\y_\\tp$ chosen for position $\\tt$ output is sampled from this distribution.\n",
    "\n",
    "Common sampling strategies:\n",
    "- Greedy sampling: the token from the Vocabulary $\\Vocab$ with maximum probability\n",
    "$$\\y_\\tp = \\Vocab_j \\text{ where } j = \\argmax{}{ \\pr{\\y_\\tp}} $$\n",
    "- Top-$n$ sampling: sample from the $n$ $\\Vocab$ tokens with highest probability in $\\pr{\\y_\\tp}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Candidate generation\n",
    "\n",
    "The simplest approach to generating candidate sequences:\n",
    "- Seed with the special `[START]` token\n",
    "- Top-n sampling from probability distribution for sequences of length $256$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Top-n sampling favors high probability sequences, which limits diversity.\n",
    "\n",
    "To increase diversity\n",
    "- access the logits (the \"scores\" for each vocabulary item\n",
    "- which are converted each logit $l_j$ to probability $\\pr{\\Vocab_j}$ using the Softmax\n",
    "$$\n",
    "\\pr{ \\Vocab_j } = \\text{softmax}( l_j ) = \\frac{ \\exp(l_j) }{\\sum_{j'=1}^{|\\Vocab|} { \\exp(l_{j'}) } }\n",
    "$$\n",
    "- \"flatten\" the output probabilities through a softmax \"temperature\" $\\tau$\n",
    "$$\\text{softmax}_\\tau ( l_j ) = \\frac{ \\exp(l_j/\\tau) }{\\sum_{j'=1}^{|\\Vocab|} { \\exp(l_{j'}/\\tau) } }$$\n",
    "    - high temperature $\\tau$ reduces the probability spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But the simplest seed `[START]` creates sequences that are not conditioned on any \"real\" text.\n",
    "\n",
    "The results may be highly likely in distribution yet not produce an actual training example\n",
    "- which is a biased choice: using whatever selection criteria for examples was applied\n",
    "    - e.g., unlikely to unconditionally generate a sequence with highly technical or foreign words\n",
    "\n",
    "The authors also try seeds from actual Internet text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors also observe on the influence of Prompt Engineering in order to get an LLM\n",
    "to generate good sequences.\n",
    "\n",
    "Thus, the naive seeds (no Prompt Engineering) are less likely to generate training examples\n",
    "- so estimates for how many examples a model has memorized are *under*-estimates\n",
    "\n",
    "Here is a prompt that successfully recalls an individual's contact info:\n",
    "\n",
    "<table>\n",
    "\n",
    "<img src=\"images/Extracting_examples_from_LLM_attack.png\">\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2012.07805.pdf#page=1\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Candidate filtering\n",
    "\n",
    "A large number of candidates needs to be reduced to those most likely to be training examples.\n",
    "\n",
    "A variety of metrics are used for filtering.\n",
    "\n",
    "- Low Perplexity $\\mathcal{P}$\n",
    "$$\n",
    "\\mathcal{P}( \\y_{[1:T]} ) = \\exp( - \\frac{1}{T}  \\sum_{\\tt=1}^T { \\log{ \\pr{ \\y_\\tp | \\y_{([1:\\tt-1])} } } } )\n",
    "$$\n",
    "\n",
    "    - $\\pr{ \\y_\\tp | \\y_{([1:\\tt-1])} }$ is high for high confidence output $\\y_\\tp$\n",
    "    - thus, for high confidence sequences\n",
    "        - sum is high\n",
    "        - perplexity is low (negative of sum is a power) for highly confident sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Comparison against another LLM\n",
    "    - High confidence does not equate to likelihood of being a training example\n",
    "        - may just be a common sequence\n",
    "            - the sequences over the digits 0 through 9\n",
    "    - If a second LLM also computes high probability for a sequence\n",
    "        - the sequence is less likely to be a training example\n",
    "        - more likely to be a common sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "In the experiments in the paper\n",
    "- about $33 \\%$ of the candidates were correctly identified as being memorized\n",
    "\n",
    "Remember: the candidates were chosen as having indicators of being memorized\n",
    "- so this is the fraction of *suspicious* outputs that were, in fact, memorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some of the memorized content is not surprising, as the detailed analysis shows\n",
    "\n",
    "<table>\n",
    "<img src=\"images/Extracting_examples_from_LLM_categorization.png\">\n",
    "    \n",
    "Attribution: https://arxiv.org/pdf/2012.07805.pdf#page=9\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many examples belong to \"often-quoted\" categories\n",
    "- Software/Publication licenses\n",
    "    - required to be quoted verbatim as condition of use\n",
    "- News headlines\n",
    "- URL's\n",
    "- Passages from religious texts, Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But there were also some potentially harmful categories\n",
    "- Names of individuals (other than those in the news)\n",
    "- Contact info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Can we guarantee the absence of leaking ?\n",
    "\n",
    "A naive approach (which **does not** guarantee against harm) is *anonymization*.\n",
    "- For each example\n",
    "    - replace Personally Identifying Information  (PII - such asname, id number, address, phone number) by random values\n",
    "    - keep the sensitive attributes in the example\n",
    "\n",
    "The scenario that causes harm is one in which an adversary has *auxiliary* (external) information\n",
    "- an \"adversary\" is able to link PII (via external information) to some of the attributes (sensitive or not) in an example\n",
    "- DOB, gender and zipcode are together sufficient in may cases\n",
    "- thereby \"unmasking\" the anonymized PII attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- Crypto-currency (e.g., Bitcoin) is anonymous but with weak protection against harm\n",
    "    - Your id for blockchain transactions is not PII\n",
    "    - Having an account at a crypto exchange is **not** anonymous (KYC)\n",
    "    - exchanging your Bitcoin for cash at an exchange\n",
    "        - links information external to the Blockchain (the PII you disclose to the exchange)\n",
    "        - to your transactions on the blockchain\n",
    "- Want to know if your favorite celebrity is [a good tipper of taxi drivers ?](https://www.gawker.com/the-public-nyc-taxicab-database-that-accidentally-track-1646724546)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is there a way that\n",
    "- we, as individuals\n",
    "- or the custodians of our sensitive data\n",
    "- can contribute our sensitive data as training examples\n",
    "- with a *guarantee* of no harm ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Differential Privacy\n",
    "\n",
    "*Differential Privacy* is a method that provides a *mathematical* guarantee of privacy.\n",
    "\n",
    "It is beyond the scope of this module to describe in detail but we summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Privacy\n",
    "\n",
    "Rather than just focusing just on the behavior of the trained model, we focus on the *entire process* \n",
    "- training: data and training algorithm \n",
    "- inference\n",
    "\n",
    "We refer to the entire process as a *mechanism*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember: training has multiple non-deterministic steps that affect the weights of the final model\n",
    "- randomized split into train/test\n",
    "- randomization of minibatches\n",
    "\n",
    "Each different random choice will potentially change the final trained model.\n",
    "\n",
    "There are also variants of Gradient Descent, each with choices of hyper-parameters\n",
    "- e.g., learning rate (which may be a schedule rather than a constant)\n",
    "\n",
    "Hence, the final weights of the model are dependent on many aspects of the process.\n",
    "\n",
    "We want to make a mathematical statement about privacy that is relative to the entire mechanism rather\n",
    "than just the final (random) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [$\\epsilon$ Differential Privacy ($\\epsilon-DP$)](https://arxiv.org/pdf/1607.00133.pdf#page=2)\n",
    "\n",
    "In Machine Learning, the *Membership Inference* task \n",
    "- is a binary classification\n",
    "- as to whether a particular example $e$ is in the training dataset for an ML model $M$.\n",
    "\n",
    "Our ultimate goal is to be able to bound the probability of being able to determine\n",
    "Membership Inference for any example, given an ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The mathematical notion of *Differential Privacy* is a way of mathematically stating\n",
    "whether a *mechanism* leaks data.\n",
    "\n",
    "Let \n",
    "- $\\mathcal{M}: \\mathcal{D} \\mapsto \\mathcal{R}$ be a mechanism\n",
    "- $\\mathcal{M}(D_1)$ be the mechanism $\\mathcal{M}$ trained on set of examples $D_1$\n",
    "- $\\mathcal{M}(D_2)$ be the mechanism $\\mathcal{M}$ trained on set of examples $D_2$ differing from $D_1$ by a single example $e$\n",
    "$$\n",
    "    D_2 = D_1 \\cup \\{ e \\}\n",
    "    $$\n",
    "- both implement functions with no argurments that have range $\\mathcal{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mechanism $\\mathcal{M}$ achieves *$\\epsilon$ Differentially Private (DP)* if\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\pr{ \\mathcal{M}(D_1) \\in S}  & \\le & \\exp(\\epsilon) * \\pr{ \\mathcal{M}(D_2) \\in S}  \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "- for all subsets $S \\subseteq \\mathcal{R}$\n",
    "- for all $D_1, D_2$ differing by a single example\n",
    "\n",
    "In words\n",
    "- the functions computed by $\\mathcal{M}$ when trained on the two nearly-similar datasets\n",
    "- are indistinguishable\n",
    "    - proportional difference in probability of the output for the two possible inputs is bounded by a factor of $\\exp(\\epsilon)$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple algebra allows us to express the equation in multiple equivalent forms\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\pr{ \\mathcal{M}(D_1) \\in S}  & \\le & \\exp(\\epsilon) * \\pr{ \\mathcal{M}(D_2) \\in S}  \\\\\n",
    "\\log { \\frac{ \\pr{ \\mathcal{M}(D_1) \\in S} }{ \\pr{ \\mathcal{M}(D_2) \\in S}  } } \n",
    "& \\le & \\epsilon & \\text{equivalently (re-arrange and take logs) }\\\\\n",
    "\\log { \\pr{ \\mathcal{M}(D_1) \\in S} }  - \\log { \\pr{ \\mathcal{M}(D_2) \\in S} } \n",
    "& \\le & \\epsilon & \\text{equivalently (log of ratio is difference of logs)} \\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, if we can prove that $\\mathcal{M}$ achieves $\\epsilon$\n",
    "- the probability of determining Membership Inference is bounded\n",
    "- the possibility of leaking a training example is bounded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The bound $\\epsilon$ is called the *privacy budget*.\n",
    "- it quantifies the privacy loss\n",
    "\n",
    "One way to make a function differentially private is to add noise to the output.\n",
    "- return\n",
    "$$\n",
    "\\mathcal{M}(D) + \\mathcal{N}(0, \\sigma)\n",
    "$$\n",
    "rather than\n",
    "$$\n",
    "\\mathcal{M}(D)\n",
    "$$\n",
    "\n",
    "- The output is no longer a deterministic function of the training set.\n",
    "- More noise is necessary for a smaller privacy budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The amount of noise depends on many aspects\n",
    "- In a multi-layer network\n",
    "    - layer $\\ll$ may implement a function achieving $\\epsilon_\\ll$-DP\n",
    "    - the privacy budget $\\epsilon$ of the composed layers is a function of $\\epsilon_\\ll$ for all layers $\\ll$\n",
    "- Allowing repeated queries may require a larger privacy budget\n",
    "    - we can estimate $\\mathcal{M}(D)$ by averaging many samples of $\\mathcal{N}(0, \\sigma^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Asides: technical details\n",
    "\n",
    "**Aside (history)**\n",
    "\n",
    "This definition was originally motivated by concern for privacy in databases\n",
    "- $D_1, D_2$ are databases differing by a single row\n",
    "- $\\mathcal{M}$ is a *query function* on the database\n",
    "    - return attributes conditional on predicates of the attributes and database\n",
    "    - this definition of query function would seem to be a concrete query with no parameters\n",
    "        - all parameter-like values are bound to specific values\n",
    "- Doing so allows the following theorem to be stated as a consequence of $D_1, D_2$ rather than the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside (relating to ML models)**\n",
    "\n",
    "Let's try to relate this to ML models.\n",
    "\n",
    "Our usual definition of an ML model is as a function from features to label.\n",
    "- for any feature vector\n",
    "- rather than for *no arguments* as in the database \"query\" above\n",
    "    - by analogy: $\\mathcal{M}(D_1), \\mathcal{M}(D_1)$ are the results of calling our ML model\n",
    "        -  on a pre-defined set of examples that are built into the function as arguments\n",
    "        - rather than passed as a parameter \n",
    "- it would be more interesting if $\\mathcal{M}(D_1), \\mathcal{M}(D_1)$\n",
    "    - did not have the single example built into the function\n",
    "    - but rather was an arbitrary parameter\n",
    "    - i.e. if both were functions from $\\mathcal{D} \\mapsto \\mathcal{R}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside (randomized functions)**\n",
    "\n",
    "$\\mathcal{M}$ can compute a *randomized* function\n",
    "- hence the range is a set\n",
    "- for example:\n",
    "    - the output of the model is sampled from a probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Differentially Private Stochastic Gradient Descent (DP-SGD)](https://arxiv.org/pdf/1607.00133.pdf#page=3)\n",
    "\n",
    "One necessary ingredient in creating models that achieve $\\epsilon$-DP \n",
    "- is to ensure that all steps in the mechanism achieve $\\epsilon$-DP.\n",
    "\n",
    "Here we discuss *Differentially Private SGD (DP-SGD)*\n",
    "- a variant of SGD that achieve $\\epsilon$-DP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Gradient Descent (for a model with weights $\\Theta$)\n",
    "- there is a per-example loss for example $i$:  $\\loss^\\ip_\\Theta$\n",
    "- a gradient of the per-example loss with respect to weights: $\\nabla_\\Theta \\loss^\\ip_\\Theta$\n",
    "- an update of weights $\\Theta$\n",
    "    - in the negative direction of the per-example loss (negative: to *minimize* loss)\n",
    "    - modulated by learning rate $\\alpha$\n",
    "    \n",
    "**Aside**\n",
    "\n",
    "$\\nabla_\\Theta \\loss^\\ip_\\Theta$ is a vector (one element per element of $\\Theta$)\n",
    "- so we should really refer to the magnitude of the *norm* \n",
    "$$\n",
    "|| \\nabla_\\Theta \\loss^\\ip_\\Theta ||_2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An example $i$ with a very large magnitude  $\\nabla_\\Theta \\loss^\\ip_\\Theta$\n",
    "- can affect the final $\\Theta$ greatly\n",
    "- potentially revealing the presence of example $i$ in the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, DP-SGD *clips* the norm of each example's gradient, returning\n",
    "$$\n",
    "\\nabla_\\Theta \\loss^\\ip_\\Theta * \\max{}(1, \\frac{ || \\nabla_\\Theta \\loss^\\ip_\\Theta ||_2}{C} )\n",
    "$$\n",
    "\n",
    "where $C$ is a bound on the maximum allowable magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Additionally: random noise is added to the clipped gradient to achieve privacy, returning\n",
    "$$\n",
    "\\nabla_\\Theta \\loss^\\ip_\\Theta * \\max{}(1, \\frac{ || \\nabla_\\Theta \\loss^\\ip_\\Theta ||_2}{C} ) + \n",
    "\\mathcal{N}(0, \\sigma^2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing Privacy in TensorFlow\n",
    "\n",
    "TensorFlow has an easy to use\n",
    "- [module](https://www.tensorflow.org/responsible_ai/privacy/tutorials/classification_privacy)\n",
    "to implement $\\epsilon$-DP.\n",
    "\n",
    "One begins by importing the modules and functions\n",
    "\n",
    "        import tensorflow_privacy\n",
    "\n",
    "        from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And specifying the use of DP-SGD as the optimizer for training\n",
    "\n",
    "        optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            num_microbatches=num_microbatches,\n",
    "            learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One can compute the privacy budget ($\\epsilon$ achieved) by\n",
    "\n",
    "        compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=train_data.shape[0],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      noise_multiplier=noise_multiplier,\n",
    "                                                      epochs=epochs,\n",
    "                                                      delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instruction following\n",
    "\n",
    "[InstructGPT paper](https://arxiv.org/pdf/2203.02155.pdf)\n",
    "\n",
    "Here is an example of Instruction Following behavior.  Given an Instruction (task description)\n",
    "\n",
    "     Instruction: Write a letter from the perspective of a cat.\n",
    "\n",
    "generate an output\n",
    "     \n",
    "        Dear [Owner],\n",
    "        I am writing to you today because I have a problem. You see, I’ve\n",
    "        been living with you for the past [Number of Years] and I love it\n",
    "        here. The food is great, the bed is comfy, and there are plenty\n",
    "        of toys to play with. But lately, I’ve noticed that you’re not as\n",
    "        affectionate towards me as you used to be. In fact, sometimes when\n",
    "        I try to cuddle up next to you on the couch, you push me away. I\n",
    "        don’t know what I did wrong, but I want things to go back to the way\n",
    "        they were. Please help me fix this.\n",
    "        I hope to hear from you soon.\n",
    "        Sincerely,\n",
    "        [Your Name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LLM's struggle with Instruction Following \n",
    "Large Language models are trained on the \"predict the next\" objective.\n",
    "\n",
    "Yet, once an LLM is sufficiently large, new capabilities emerge without further training\n",
    "via *in-context* learning\n",
    "- zero-shot learning\n",
    "- few shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are methods by which \n",
    "- one provides $k \\ge 0$ exemplars (labeled examples $\\langle \\x, \\y\\rangle$) of a new task behavior\n",
    "- as input of the pre-trained LLM\n",
    "- in order to *demonstrate* a new Target task\n",
    "- and then provide the model with an *unlabelled* example\n",
    "- and hope the model produces a label $\\hat \\y$ \n",
    "    - that is the correct label on input $\\x$ for the Target task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One task that a pre-trained LLM seems to struggle with is *Instruction Following*\n",
    "\n",
    "Consider the response of GPT-3 (left column) to some Instructions in the chart below\n",
    "\n",
    "<img src=\"images/instructGPT_vs_GPT_output.png\">\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2203.02155.pdf#page=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the first example, the prompt (in French) is\n",
    "\n",
    "    Write a short story about a frog who travels back in time to ancient Greece in French.\n",
    "\n",
    "Even if you don't understand French, you can see that each paragraph in the output is highly repetitive.\n",
    "\n",
    "In this case, the model may have followed the instruction, but not sufficiently well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the second example (explain a piece of code)\n",
    "- the output is not even answer\n",
    "- it appears to be the answer-part of a multiple-choice question\n",
    "\n",
    "In this case, the LLM did not follow the instruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fine-tuning an LLM to demonstrate Instruction Following behavior\n",
    "\n",
    "The way to extend a pre-trained model's behavior to encompass a new Target task is with Transfer Learning.\n",
    "\n",
    "The *Unsupervised Pre-Training + Supervised Fine-Tuning paradigm* is a type of Tranfer Learning\n",
    "- Adapting a Pre-Trained LLM\n",
    "- By Fine-Tuning with a small number of examples from the Target task\n",
    "\n",
    "To get a LLM to exhibit Instruction Following behavior, we need to have a dataset of examples\n",
    "that demonstrates Instruction Following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The examples in this dataset will be pairs\n",
    "- an Instruction part\n",
    "    - possibly with Additional Input\n",
    "- a Target Output part\n",
    "    - the response\n",
    "\n",
    "For example\n",
    "- an Instruction part $\\x$\n",
    "   \n",
    "   \n",
    "    Instruction: Given a word, find out its length and its number of vowels.\n",
    "    Input: Word = \"hello\"\n",
    "  \n",
    "- a Target Output part $\\y$\n",
    "    \n",
    "    \n",
    "    Output: Length = 5, Number of vowels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[InstructGPT](https://arxiv.org/pdf/2203.02155.pdf) is a pre-trained GPT-3 that has been Fine-Tuned on \n",
    "a dataset that demonstrations of Instruction Following.\n",
    "\n",
    "The chart above demonstrating Instruction Following (i.e., the one with the prompt in French to write a story)\n",
    "- compares the Instruction following of pre-trained GPT-3\n",
    "- with a Fine-Tuned GPT-3\n",
    "\n",
    "The results are more satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where do the Instruction Following demonstration examples come from ?\n",
    "\n",
    "The demonstration examples for Instruction Following\n",
    "- were manually constructed by human labelers\n",
    "\n",
    "This is a very labor-intensive process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using an LLM to generate Instruction Following examples\n",
    "\n",
    "[SELF-Instruct paper](https://arxiv.org/pdf/2212.10560.pdf)\n",
    "\n",
    "Is there an alternative to the labor-intensity of constructing Instruction Following examples by human ?\n",
    "\n",
    "The idea of the [SELF-Instruct paper](https://arxiv.org/pdf/2212.10560.pdf)\n",
    "is to use a Synthetic Data approach to constructing new examples of Instruction Following\n",
    "\n",
    "These examples are pairs of an Instruction part, and a Target Output part.\n",
    "\n",
    "The authors\n",
    "- use a *few-shot* learning approach to generate *synthetic* Instruction Following examples\n",
    "- augmenting a small number of human-constructed examples with the synthetic examples\n",
    "- using the augmented dataset to Fine Tune an LLM to better demonstrate Instruction Following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/selfinstruct_process.png\">\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2212.10560.pdf#page=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process involves multiple steps which we explain below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating the Instruction part of an Instruction-Output example\n",
    "\n",
    "The first step is to use few shot learning to generate synthetic Instructions\n",
    "- the Instruction part of an Instruction-Target Output example\n",
    "\n",
    "The synthetic Instructions are used to augment a small number of Instructions from the manually generated training dataset.\n",
    "\n",
    "Recall: few-shot learning involves creating a prompt that is the concatenation of\n",
    "- a few exemplars ($\\langle \\x, \\y \\rangle$ pairs demonstration the task)\n",
    "- an example with no label: $\\x$\n",
    "\n",
    "Here is a template for a prompt demonstrating to GPT how to create a new Instruction \n",
    "\n",
    "<img src=\"images/selfinstruct_task_generation_prompts.png\">\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2212.10560.pdf#page=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating the Output part, given an Instruction\n",
    "\n",
    "The next step is to \n",
    "- choose an Instruction (called the *Target task*) from the augmented list of Instructions \n",
    "- prompt the LLM to generate the Target Output for the target task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The prompting for the output is achieved by few-shot learning.\n",
    "- Provide $k$ exemplars\n",
    "- Followed by a line consisting of \n",
    "    - The Instruction for the Target Task\n",
    "    -with the expectation that the LLM will create an Input/Output pair\n",
    "        - that obeys the Instruction\n",
    "        - correctly relates the Input and the Output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each exemplar is an Instruction following example for some other task.\n",
    "\n",
    "That is, it is a Instruction-Target Output pair.\n",
    "    - \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For Classification tasks, the prompt might look like this\n",
    "\n",
    "    Task: Classify the sentiment of the sentence into positive, negative, or mixed\n",
    "    \n",
    "    Example 1\n",
    "    Sentence: I enjoy the flavor of the restaurant but their service is too slow.\n",
    "    Class Label: mixed\n",
    "    \n",
    "    Example 2\n",
    "    Sentence: I had a great day today. The weather was beautiful and I spent time with friends.\n",
    "    Class label: Positive\n",
    "    \n",
    "    \n",
    "    Task: Tell me if the following email is a promotion email or not.\n",
    "    \n",
    "    Email: Check out our amazing new sale! We’ve got discounts on all of your favorite products.\n",
    "    Class label: Promotion\n",
    "\n",
    "    Email: We hope you are doing well. Let us know if you need any help.\n",
    "    Class label: Not Promotion\n",
    "    \n",
    "    Task: {instruction for the target task}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The last line above contains a place holder for the Instruction of the Target Task\n",
    "- the one for which we want the LLM to create a Target Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is an example of the template from the paper\n",
    "\n",
    "<img src=\"images/selfinstruct_generated_instances.png\">\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2212.10560.pdf#page=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generating examples for Classification tasks\n",
    "\n",
    "Consider the an Instruction Following example for a Classification task\n",
    "\n",
    "    Task: Classify the sentiment of the sentence into positive, negative, or mixed\n",
    "    \n",
    "    Example 1\n",
    "    Sentence: I enjoy the flavor of the restaurant but their service is too slow.\n",
    "    Class Label: mixed\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors found that the response generated by the LLM (e.g., Classification examples)\n",
    "- were examples whose Class Label's \n",
    "- were not well-distributed among all possible labels \n",
    "\n",
    "This was attributed to the *format* of the example called *Input-first*.\n",
    "- Additional Input\n",
    "- Precedes Target Output (e.g., `Class Label:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the format was changed to *output-first*\n",
    "- Target Output \n",
    "- precedes Additional Input\n",
    "\n",
    "the Classification examples generated had Class Label's that were less biased to one label\n",
    "\n",
    "\n",
    "     Task: Classify the sentiment of the sentence into positive, negative, or mixed\n",
    "\n",
    "     Example 1\n",
    "        Class Label: mixed\n",
    "        Sentence: I enjoy the flavor of the restaurant but their service is too slow.\n",
    "        \n",
    "\n",
    "        Example 2\n",
    "        Class label: Positive\n",
    "        Sentence: I had a great day today. The weather was beautiful and I spent time with friends.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is an example of Prompt Engineering\n",
    "- In-context learning seems very sensitive to the format of prompts\n",
    "- There is a skill of engineering a prompt to elicit the desired behavior\n",
    "\n",
    "This feels similar to the idea behind Chain of Thought prompting\n",
    "- by presenting `Class Label` first\n",
    "- the model seems better conditioned to generate a less biased distribution of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Related work\n",
    "\n",
    "A related [paper](https://arxiv.org/pdf/2210.11610.pdf) adds some interesting ideas.\n",
    "\n",
    "The first idea relates to the construction of the exemplars\n",
    "- use Chain of Though (CoT) exemplars as demonstrations of the task\n",
    "    - for example generation\n",
    "\n",
    "CoT prompts have been shown to increase the likelihood of generating a correct response\n",
    "- by explicitly asking for \"step by step\" reasoning to be included\n",
    "- rather than just outputting the \"answer\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But even with step by step reasoning, a wrong answer may be output.\n",
    "\n",
    "The other idea adapted by the authors is *multiple reasoning paths*\n",
    "- sample multiple outputs for each question\n",
    "- extract the \"answer\" part (i.e., ignore the step by step part) from the output\n",
    "- the answer that occurs most frequently among the multiple answers is deemed more likely to be correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
